{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center\">\n",
    "<strong>WEB SCRAPING - REPOSITORIO DE UNSA</strong>\n",
    "</h1>\n",
    "\n",
    "**Nombre:** Oliverio Pichardo Diestra\n",
    "\n",
    "**Descripción del caso:**\n",
    "Se quiere obtener información estructurada de las tesis de pregrado y posgrado de Estadística \n",
    "Informática o de carreras similares desarrolladas a nivel  nacional, a través de los repositorios \n",
    "institucionales de las universidades.\n",
    "\n",
    "**Especificaciones:**\n",
    "1. Debido a que se menciona carreras similares y no se especifica terminología alguna para ello, \n",
    "entonces se procede a considerar lo establecido por INEI en su *Clasificador Nacional de Programas e Instituciones de \n",
    "Educación Superior Universitaria, Pedagógica, Tecnológica y Técnico Productiva, 2018*. (Ver Tabla 1)\n",
    "\n",
    "    **Tabla 1**  \n",
    "    *Programas similares a Estadística a nivel de profesional (pregrado)*\n",
    "    | CÓDIGO |                      PROGRAMA                     |\n",
    "    |:------:|:-------------------------------------------------:|\n",
    "    | 542016 |                    Estadística                    |\n",
    "    | 542026 |             Estadística e informática             |\n",
    "    | 542036 |              Estadística informática              |\n",
    "    | 542046 | Estadística para la gestión de servicios de salud |\n",
    "    | 542056 |               Ingeniería estadística              |\n",
    "    | 542066 |        Ingeniería estadística e informática       |\n",
    "    | 542076 |         Ingeniería estadística informática        |\n",
    "    | 542996 |           Otros programas en estadística          |\n",
    "\n",
    "    *Nota: Adaptado del \"Clasificador Nacional de Programas e Instituciones de \n",
    "    Educación Superior Universitaria, Pedagógica, Tecnológica y Técnico Productiva, 2018\" por INEI.*\n",
    "\n",
    "2. El repositorio escogido es el de la Universidad Nacional de San Agustín de Arequipa (UNSA). \n",
    "El respectivo url de dicho repositorio es:  http://repositorio.unsa.edu.pe/browse?type=dateissued **(url-principal)**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Web Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Librerías, módulos o paquetes\n",
    "A fin de realizar la extracción de datos pertinentes, se emplearán las siguientes librerías, módulos o paquetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargando librerías, módulos o paquetes pertinentes\n",
    "# %pip install requests\n",
    "# %pip install pandas\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Funciones\n",
    "Se definirán dos funciones las cuales tendrán los siguientes objetivos respectivamente:\n",
    "1. Extraer todas las urls de la diversas páginas de la *url-principal* en las que se encuentran las tesis de pregrado y posgrado. Tales urls serán denominadas *url-página*.\n",
    "2. De cada *url-página*, extraer las urls que contienen la información específica de cada una las tesis. A estas les denominaremos *url-publicación*.\n",
    "\n",
    "A fin de comprender mejor lo mencionado, veamos la estructura jerárquica:\n",
    "1. Nivel 1 (url-principal): http://repositorio.unsa.edu.pe/browse?type=dateissued\n",
    "2. Nivel 2 (url-página): Es una subpágina de la url-principal que contiene en promedio 20 publicaciones. Hasta la fecha, en total, existen 682 subpáginas.\n",
    "3. Nivel 3 (url-publicación): Es una subpágina de una url-página que contiene la información específica de una determinada publicación. Hasta el momento, contiene en promedio 28 descriptores de la respectiva publicación."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Extracción de todas las url-página de la url-principal\n",
    "A fin de realizar la extracción de todas las url-página de la url-principal, se identificó un patrón de url que sirve para generar todas las url-página. A esa se le denominará *url-base*.\n",
    "A continuación, se define una función en la que:\n",
    "1. se identifica la url-base,\n",
    "2. se crea una lista de los caracteres que se deben reemplazar en el url-base para generar todas la url-página y\n",
    "3. se generan las url-página mediante la sustitución de unos caracteres específicos de la url-base por los elementos la lista generada en el punto 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando una función que permite extraer todas las url-página\n",
    "def all_pages(url):\n",
    "    # Descargar el contenido de la url-principal\n",
    "    page = requests.get(url)\n",
    "    # Crear un objeto BeautifulSoup a partir del contenido de la página\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    # Encontrar el tag a que contiene la url-base de las url-página\n",
    "    pages = soup.find_all(\"a\", {\"class\":\"next-page-link\"})\n",
    "    # Extraer el url-base de las url-página\n",
    "    pages_soup = BeautifulSoup(str(pages), 'html.parser')\n",
    "    a_tags = pages_soup.find_all('a') # find all anchor elements\n",
    "    base_href = [\"http://repositorio.unsa.edu.pe/\" + a_tag['href'] for a_tag in a_tags][0]\n",
    "    # Crear lista de límites inferiores de cada url-página\n",
    "    string_pages = soup.find_all(\"p\", {\"class\":\"pagination-info\"})\n",
    "    matches = re.findall(\"\\d+\", str(string_pages[0]))\n",
    "    lower_limit_max = (max([int(item) for item in matches]) // 20) * 20\n",
    "    lower_limits = list(range(0, lower_limit_max + 20, 20))\n",
    "    # Crear lista de las url-página de la url-principal\n",
    "    hrefs_all = [re.sub(r\"offset=\\d+\", \"offset=\" + str(pag), base_href) for pag in lower_limits]\n",
    "    return hrefs_all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Extracción de todas las url-publicación de una url-página\n",
    "A fin de realizar la extracción de todas las url-publicación de una url-página, se identificó el tag que contiene la expresión extra añadir en http://repositorio.unsa.edu.pe que permite acceder hasta la información de una determina publicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creando una función que permite extraer todas las url-publicación\n",
    "def links_scrapy(url):\n",
    "    # Descargar el contenido de la url-página\n",
    "    page = requests.get(url)\n",
    "    # Crear un objeto BeautifulSoup a partir del contenido de la página\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    # Encontrar el tag padre de las publicaciones que contiene su respectivo url-publicación\n",
    "    publication = soup.find_all(\"div\", {\"class\":\"artifact-title\"})\n",
    "    # Extraer los url-publicación de los tags hijos respectivos\n",
    "    publication_soup = BeautifulSoup(str(publication), 'html.parser')\n",
    "    a_tags = publication_soup.find_all('a')\n",
    "    hrefs = [\"http://repositorio.unsa.edu.pe\" + a_tag['href'] + \"?show=full\" for a_tag in a_tags]\n",
    "    return hrefs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Aplicación: Extracción de las url-publicación del repositorio\n",
    "A continuación, se obtendrá todas las url-publicación del repositorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTA: no se recomienda ejecutar esta celda, ya que toma algo de tiempo obtener un output (9 min aprox.)\n",
    "# Extraer todas las url-página de la url-principal\n",
    "## every_page: es una lista que contiene las url-página\n",
    "every_page = all_pages(\"http://repositorio.unsa.edu.pe/browse?type=dateissued\")\n",
    "# Extraer todas las url-publicación de cada una de las url-pagina\n",
    "## publications_links: es un a lista que contiene todas las url-publicación\n",
    "## publications_links se obtuvo de aplanar (flatten) la lista de listas de url-publicación de cada una de las url-página\n",
    "publications_links = list(itertools.chain(*[links_scrapy(url) for url in every_page]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la ejecución de lo anterior para los dos primeras url-página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJEMPLO DE EJECUCIÓN\n",
    "# En este ejemplo solo se considera las dos primeras páginas del repositorio\n",
    "every_page = all_pages(\"http://repositorio.unsa.edu.pe/browse?type=dateissued\")\n",
    "publications_links = list(itertools.chain(*[links_scrapy(url) for url in every_page[0:2]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Extracción de la información solicitada\n",
    "Se procederá a extraer la información de cada una de las url-publicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encabezado para la extracción de la tabla\n",
    "headers = {\"User-agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n",
    "# Crear las listas donde se almacenará la información de las tesis\n",
    "anios, instituciones, titulos, autores, contribuidores, grados, resumenes = [], [], [], [], [], [], []\n",
    "# Añadir un contador que será de utilidad para colocar un dato vacío en el caso de que\n",
    "# la tesis no cuente con la información que se requiera\n",
    "n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(publications_links)):\n",
    "    respuesta = requests.get(publications_links[i], headers=headers)\n",
    "    tabla_tesis = pd.read_html(respuesta.content, encoding = 'utf8')\n",
    "    tabla_1 = tabla_tesis[0]\n",
    "    # Seleccionar el dato de la fecha de tesis de cada link y añadir en su respectiva lista\n",
    "    for j in range(len(tabla_1)):\n",
    "        if tabla_1.iloc[j,0] == 'dc.date.issued':\n",
    "            año=tabla_1.iloc[j,1]\n",
    "            anios.append(año)\n",
    "            n = n + 1\n",
    "    if n == 0:\n",
    "        anios.append(\" \")\n",
    "    else:\n",
    "        n = 0\n",
    "    #Seleccionar el dato de la institución de cada link y añadir en su respectiva lista\n",
    "    for j in range(len(tabla_1)):\n",
    "        if tabla_1.iloc[j,0] == 'dc.publisher':\n",
    "            institucion=tabla_1.iloc[j,1]\n",
    "            instituciones.append(institucion)\n",
    "            n = n + 1\n",
    "    if n == 0:\n",
    "        instituciones.append(\" \")\n",
    "    else:\n",
    "        n = 0\n",
    "    # seleccionar el dato del titulo de cada link y añadir en su respectiva lista\n",
    "    for j in range(len(tabla_1)):        \n",
    "        if tabla_1.iloc[j,0] == 'dc.title':\n",
    "            titulo=tabla_1.iloc[j,1]\n",
    "            titulos.append(titulo)\n",
    "            n = n + 1\n",
    "    if n == 0:\n",
    "            titulos.append(\" \")\n",
    "    else:\n",
    "        n = 0\n",
    "    #seleccionar el dato del autor de tesis de cada link y añadir en su respectiva lista\n",
    "    for j in range(len(tabla_1)):\n",
    "        if tabla_1.iloc[j,0] == 'dc.contributor.author':\n",
    "            autor=tabla_1.iloc[j,1]\n",
    "            autores.append(autor)\n",
    "            n = n + 1\n",
    "    # En caso exista más de un autor, sólo seleccionar el primero\n",
    "            break\n",
    "    if n == 0:\n",
    "            autores.append(\" \")\n",
    "    else:\n",
    "        n = 0 \n",
    "    #seleccionar el dato del grado de tesis de cada link y añadir en su respectiva lista\n",
    "    for j in range(len(tabla_1)):\n",
    "        if tabla_1.iloc[j,0] == 'thesis.degree.name':\n",
    "            grado=tabla_1.iloc[j,1]\n",
    "            grados.append(grado)\n",
    "            n = n + 1\n",
    "    if n == 0:\n",
    "            grados.append(\" \")\n",
    "    else:\n",
    "        n = 0\n",
    "    #seleccionar el dato del asesor de tesis de cada link y añadir en su respectiva lista\n",
    "    for j in range(len(tabla_1)):\n",
    "        if tabla_1.iloc[j,0] == 'dc.contributor.advisor':\n",
    "            contribuidor=tabla_1.iloc[j,1]\n",
    "            contribuidores.append(contribuidor)\n",
    "            n = n + 1\n",
    "    #En caso exista más de un asesor, sólo seleccionar el primero\n",
    "            break\n",
    "    if n == 0:\n",
    "            contribuidores.append(\" \")\n",
    "    else:\n",
    "        n = 0\n",
    "    #seleccionar el dato del resumen de tesis de cada link y añadir en su respectiva lista\n",
    "    for j in range(len(tabla_1)):\n",
    "        if tabla_1.iloc[j,0] == 'dc.description.abstract':\n",
    "            resumen=tabla_1.iloc[j,1]\n",
    "            resumenes.append(resumen)\n",
    "            n = n + 1\n",
    "    if n == 0:\n",
    "            resumenes.append(\" \")\n",
    "    else:\n",
    "        n = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Almacenamiento de la información solicitada\n",
    "Se procederá a almacenar la información de cada una de las url-publicación en un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un data frame donde se almacenará todos los datos de las publicaciones\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Asignar cada lista a una determinada columna del dataframe\n",
    "df['instituciones'] = instituciones\n",
    "df['titulos'] = titulos\n",
    "df['autores'] = autores\n",
    "df['contribuidores'] = contribuidores\n",
    "df['resumenes'] = resumenes\n",
    "df['grados'] = grados\n",
    "df['anios'] = anios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Exportación de la información solicitada\n",
    "Se procederá a exportar la información solicitada en un archivo .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eec1c590e2f34e9a299b6d726f651d02256ffc6f526c72002153c3ef3807bbd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
