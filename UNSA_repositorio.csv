instituciones,titulos,autores,contribuidores,resumenes,grados,anios
Universidad Nacional de San Agustín de Arequipa,Construcción automática y análisis de Modelos de Espacios de Palabras de n-gramas y su aplicación a tareas de procesamiento de lenguaje natural,"Cornejo Aparicio, Víctor Manuel","Tejada Cárcamo, Javier","La presente tesis tiene por objetivo mejorar la calidad de vocablos relacionados semánticamente mediante la construcción automática y análisis de Modelos de Espacios de Palabras basados en n-gramas. Este método debe incluir vocablos que a su vez deben mejorar la precisión de tareas de procesamiento de lenguaje natural, específicamente la clasificación de textos, para ello se emplearon modelos ya existentes como base de conceptualización y se implementaron mejoras en el pre-procesamiento de los textos, tales como la extracción de verbos y sustantivos, posteriormente se trabajó la clasificación a tres niveles de n-gramas (monogramas, digramas y digramas ordenados horizontalmente), luego se efectuaron los experimentos con el corpus estandarizado “corpora Reuters 21578”, del cual se seleccionaron las ocho categorías más relevantes con las que se obtuvo un nivel de precisión del orden del 84.17%, con lo que se superó el 83% de precisión prevalente, lo cual permitió validar la propuesta.",Doctor en Ciencias de la Computación,2013
Universidad Nacional de San Agustín de Arequipa,Recuperación de imágenes por contenido basado en regiones con Retroalimentación por Relevancia,"Velazco Paredes, Yuber Elmer","Patiño Escarcina, Raquel E.","En la actualidad la implementación de sistemas de recuperación de imágenes basadas en contenido (CBIR) ha permitido implementar múltiples aplicaciones en diversas áreas mostrando buenos resultados, así mismo, en éstos sistemas una de sus principales tareas lo constituye la extracción de características de las imágenes , tarea que puede convertirse en extensa y a su vez compleja, dado que una imagen está formada por un conjunto de píxeles, sin embargo, muchos investigadores han demostrado que es difícil obtener efectos satisfactorios en el análisis de una imagen usando sólo algoritmos simples basados en píxeles. Por ello, en el presente trabajo de investigación se ha utilizado los textones, que representan agrupaciones de píxeles formando un conjunto de patrones emergentes que comparten propiedades comunes sobre toda la imagen, los cuales pueden discriminar características de color y textura simultáneamente. El caso de estudio para el presente trabajo son las imágenes biológicas de huevos de helmintos las cuales han sido identiﬁcadas y clasiﬁcadas según la especie a la que pertenece. La innovación en esta investigación es el proceso de descubrir la forma de los nuevos textones que están presentes en las imágenes de los huevos de los helmintos con el ﬁn de que sirvan para la discriminación e identiﬁcación de las diferentes especies, así mismo esta técnica propuesta ha demostrado ser útil en otros Bancos de Imágenes.",Doctor en Ciencias de la Computación,2014
Universidad Nacional de San Agustín de Arequipa,Caracterización basado en histograma multitexton aplicado a Imágenes Biológicas,"Flores Quispe, Roxana","Beltrán Castañón, Cesar A.","En la actualidad la implementación de sistemas de recuperación de imágenes basadas en contenido (CBIR) ha permitido implementar múltiples aplicaciones en diversas áreas mostrando buenos resultados, así mismo, en éstos sistemas una de sus principales tareas lo constituye la extracción de características de las imágenes , tarea que puede convertirse en extensa y a su vez compleja, dado que una imagen está formada por un conjunto de píxeles, sin embargo, muchos investigadores han demostrado que es difícil obtener efectos satisfactorios en el análisis de una imagen usando sólo algoritmos simples basados en píxeles. Por ello, en el presente trabajo de investigación se ha utilizado los textones, que representan agrupaciones de píxeles formando un conjunto de patrones emergentes que comparten propiedades comunes sobre toda la imagen, los cuales pueden discriminar características de color y textura simultáneamente. El caso de estudio para el presente trabajo son las imágenes biológicas de huevos de helmintos las cuales han sido identiﬁcadas y clasiﬁcadas según la especie a la que pertenece. La innovación en esta investigación es el proceso de descubrir la forma de los nuevos textones que están presentes en las imágenes de los huevos de los helmintos con el ﬁn de que sirvan para la discriminación e identiﬁcación de las diferentes especies, así mismo esta técnica propuesta ha demostrado ser útil en otros Bancos de Imágenes.",Doctor en Ciencias de la Computación,2014
Universidad Nacional de San Agustín de Arequipa,Método de cálculo de precio de opción en commodities usando plataforma de computación de alto desempeño,"Carrasco Bocangel, Julio César","Tupac Valdivia, Yván Jesús","En este trabajo se propone un método de cálculo de valor de precio de opciones commodities utilizando la plataforma de computación paralela CUDA, donde se muestra la ganancia en tiempo computacional con respeto a implementaciones convencionales en CPU. Este modelo es aplicado en el problema de análisis de riesgo de variables hidroenergéticas, caso Cuenca del Río Chili, donde se emplean metodologías de calculo de opciones de precio mediante árboles trinomiales para calcular los precios de las variables commodities y poder tomar la decisión óptima del uso del recurso hídrico bajo incertidumbre. La implementación de estas metodologías en ambiente GPU es considerablemente más eficiente en la medida en que aumente la complejidad del análisis de datos agregando una mayor aceleración.",Doctor en Ciencias de la Computación,2015
Universidad Nacional de San Agustín de Arequipa,Soluciones aproximadas para algoritmos escalables de minoración de datos en dominios complejos,"Ocsa Mamani, Alexander Víctor", ,"La creciente disponibilidad de datos en diferentes ámbitos ha motivado el desarrollo de técnicas para el descubrimiento de conocimientos en grandes volúmenes de datos complejos. Un trabajo reciente muestra que la búsqueda del vecino más cercano en dominios de datos complejos es un importante campo de investigación en muchas tareas de minería de datos. Para resolver el problema de la búsqueda de los vecinos más cercanos se han propuesto muchos enfoques para reducir los efectos de la maldición de la alta dimensionalidad de los datos. Aunque se han propuesto muchos métodos exactos y aproximados, el modelo de programación impone restricciones sobre el rendimiento en la CPU para estos tipos de soluciones. Una forma de mejorar el tiempo de ejecución de las técnicas de recuperación y extracción de datos en varias órdenes de magnitud es el empleo de las nuevas arquitecturas de programación paralela, como CUDA. En ese contexto, este trabajo presenta una propuesta para búsquedas kNN basado en una técnica Hashing e implementaciones paralelas en CUDA. La técnica propuesta está basado en el esquema de indexación LSH, o sea, usa proyecciones en subespacios. LSH es una solución aproximada y tiene la ventaja de permitir consultas de costo sublinear para datos en altas dimensiones. Usando implementaciones masivamente paralelas se mejoro tareas de minería de datos. Específicamente, fueron desarrollados soluciones de alto desempeño para la identificación de Motifs basado en implementaciones paralelas de consultas kNN. Las implementaciones masivamente paralelas en CUDA permitieron ejecutar estudios experimentales sobre grandes conjuntos de datos reales y sintéticos. La validación de desempeño usando una GeForce GTX470 resulto en un aumento de desempeño de hasta 7 veces, en media sobre el estado del arte en búsquedas por similitud e identificación de Motifs.",Ingeniero de Sistemas,2015
Universidad Nacional de San Agustín de Arequipa,Método de agrupamiento no supervisado para el procesamiento del lenguaje natural utilizando medidas de similitud asimétricas y propiedades paradigmáticas,"Santisteban Pablo, Julio Omar","Tejada Carcamo, Javier Leandro","Una de las tareas más comunes para el ser humano, pero de con una alta complejidad es la agrupación y clasificación. Por otro lado, la debilidad del ser humano es la capacidad de procesar altas cantidades de datos y de forma rápida, característica propia de los computadores. Hoy en día se generan grandes cantidades de datos en el Internet, datos de distintos tipos y con diferentes objetivos. Para esto se necesitan de algoritmos de agrupación que nos permitan identificar los distintos grupos y características de estos grupos, de forma automática sin conocimiento previo. Por otro lado, es importante definir con claridad qué medida de similitud se utilizará en el proceso de agrupación, la gran mayoría de las medidas de agrupación se enfocan en un aspecto simétrico. En la presente tesis se propone una novedosa medida de similitud asimétrica, Coeficiente d Similitud Unilateral Jaccard (uJaccard), similitud no es igual entre dos objetos uJaccard(a,b) ≠ uJaccard(b,a). Así también se presenta una similitud asimétrica con pesos Coeficiente Ponderado de Similitud Unilateral Jaccard, la cual mide el nivel de incertidumbre entre dos objetos. Así también en esta tesis se propone una nueva propiedad de grafos, la propiedad paradigmática la cual considera la equivalencia regular como característica fundamental y por último se propone un algoritmo de agrupación PaC, por sus siglas en inglés Paradigmatic Clustering, el cual incorpora la uJaccard y la propiedad paradigmática. Se ha realizado evaluaciones extensivas con datos pequeños, reales, sintéticos y se ha procesado 3 grandes corpus. Se ha demostrado que PaC es un algoritmo que sobre pasa los resultados de algoritmos de agrupación del estado del arte. Más aun PaC es un algoritmo capas de ser ejecutado de forma paralela, distribuida, incremental y en flujo, características que se necesitan para el procedimiento de grandes cantidades de datos y de constante generación de datos",Doctor en Ciencias de la Computación,2016
Universidad Nacional de San Agustín de Arequipa,Un enfoque híbrido para la clasificación de imágenes de resonancia magnética del cerebro,"Limache Calatayud, Roxana Evelyn", ,"El tumor cerebral es una de las principales causas de mortalidad entre niños y adultos en todo el mundo. Un tumor es una masa de tejido que crece fuera de control, los tumores pueden ser benignos o malignos (cáncer), dependiendo de la rapidez de su crecimiento y de si logran resecarse o curarse mediante el tratamiento neuro-quirurgico. Para diagnosticar un cáncer se realizan diferentes pruebas como radiografías, tomografías, ecografías o resonancias magnéticas. mediante estas pruebas pueden detectarse zonas con alta sospecha tumoral, cuyo diagnostico debe confirmarse mediante la realización de una biopsia. Estas imágenes no son fáciles de interpretar, lo que provoca que el profesional encargado de analizarlas, a pesar de su experiencia no sea capaz de detectar en ellas un porcentaje importante de tumores. Una posibilidad para mejorar el diagnostico consiste en utilizar sistemas de diagnostico asistido por computador (CAD). Un CAD analiza la imagen medica y trata de detectar zonas sospechosas de contener alguna anomalía. Entonces el radiólogo va poder interpretar con menos dificultad la información contenida en la imagen medica. El diagnostico asistido por computador es aún una tecnología muy joven, es por ello que en esta tesis se ha propuesto una técnica hibrida formado por una red neuronal y un algoritmo gen ético para detectar en una imagen de resonancia magnética si tiene o no una anomalía. Para la etapa de extracción de características se ha utilizado la técnica de Gabor, y para la etapa de clasificación se va usar una red neuronal MLP (perceptron multicapa) con un algoritmo gen ético simple. Los resultados de aplicar el enfoque a la data de prueba muestran ser prometedores",Ingeniero de Sistemas,2016
Universidad Nacional de San Agustín de Arequipa,Modelo hibrido para diagnóstico de cáncer pulmonar caso: Células no pequeñas,"Chire Saire, Josimar Edinson", ,"La formación de películas comestibles se desarrolló a partir de goma de Tara (GT), goma Xantan (GT), almidón y con la adición de glicerol como plastificante. Se determinó como variable independiente la relación de goma Tara/Xantan (0, 20, 40, 60, 80 y 100%), y, la concentración de mezcla de gomas totales (0.5% constante) como parámetro. Para la parte experimental se tomaron dos parámetros adicionales: el almidón de 1.5% y el glicerol de 3% y como solvente se empleó agua destilada. Para determinar la composición de cada película (60ml en placas 20x20cm), se realizó la preparación por separado de cada una de las muestras obteniendo así soluciones liquidas para su mezcla. Para finalizar, las películas resultantes se evaluaron por triplicado para cada muestra y los valores experimentales fueron promediados para relacionar sus propiedades con el porcentaje de Tara/Xantan. Se determinó la evaluación de las propiedades de las películas según norma; la medición del grosor (ASTM D882-10) y la medición de elongación (fórmula), donde se obtuvo un mínimo en la película N°2 al 20% de tara y un máximo en la película N°5 con 80% de Tara. En cambio, parece que la relación Tara/Xantan no afecta el módulo de Young ni el esfuerzo de tracción, tampoco el porcentaje de elasticidad en su punto de corte, pues presentan comportamientos iguales para las películas N°2 y N°5, las cuales tienen los puntos más altos. En la evaluación de la permeabilidad (ASTM E96/E96M-05), la película N°2, que contiene 20% de Tara, posee la menor permeabilidad igual a 1.08E-5 ; por lo contrario, la película N°5 con 80% de goma de Tara presenta la mayor permeabilidad de 2.04E-5 , lo que evidencia que la mayor concentración de goma de Tara favorece la permeabilidad del vapor de agua a través de la película. En la evaluación de la viscosidad de la solución filmogénica correlacionada por el modelo de Herschel & Bulkley, se observa que a medida que aumenta la concentración de la solución Tara/Xantan aumenta el umbral de fluencia y disminuye significativamente su coeficiente de consistencia y su índice de flujo. Para las películas puras de Tara y Xantan (100%) aumenta su coeficiente y disminuye su fluencia en relación al porcentaje; por otro lado, para la película N°4 que representa el 60% Tara/Xantan su coeficiente disminuye mientras su fluencia aumenta. Para la evaluación en el microscopio óptico, la caracterización morfológica permitió identificar las irregularidades en la superficie de las películas, resultando que aquellas que poseen entre 80 y 100% de goma de Tara en la mezcla Tara/Xantan son más uniformes.",Ingeniero de Sistemas,2016
Universidad Nacional de San Agustín de Arequipa,Reconocimiento de Palabras en Manuscritos Históricos Basado en Aprendizaje Online,"Meza Lovón, Graciela Lecireth","Ochoa Luna, José Eduardo","Se propone un método de reconocimiento online para la transcripción de palabras de manuscritos históricos, el cual incluye tres etapas: preprocesamiento, que emplea técnicas para segmentar el documento en imágenes de palabras; extracción de características, que usa información basada en el gradiente; y reconocimiento propiamente dicho, donde se concentran las principales contribuciones de esta tesis. En esta última etapa, se propusieron dos extensiones de la SVM-Online: la primera permite que el modelo opere con conjuntos de datos de más de dos clases; la segunda permite que el modelo emita salidas probabilísticas y evita la segmentación del documento a nivel de caracter. El modelo propuesto, llamado Reconocedor Basado en Grafos, representa cada imagen a ser reconocida como un grafo direccionado, cuyas aristas están relacionadas a una subimagen y a una probabilidad. En función de ésta se estima, para cada arista, un costo que es empleado por el algoritmo Dijkstra Modificado, también propuesto en esta tesis, para emitir la transcripción de la imagen en cuestión. Los experimentos fueron realizados con tres conjuntos de datos. En base a los experimentos realizados, podemos concluir que los resultados obtenidos son satisfactorios en relación a otras propuestas de transcripción de textos presentadas en la literatura.",Doctor en Ciencias de la Computación,2017
Universidad Nacional de San Agustín de Arequipa,Propuesta de método de formulación del plan estratégico de tecnologías de la información y comunicaciones en Empresas del Estado. caso: Empresa de Generación Eléctrica XYZ,"Estremadoyro Escobar, Edwin Eduardo","Carrasco Bocangel, Julio César","El método de Planeamiento Estratégico de TIC aplicado para el Área de Informática de la Empresa de Generación Eléctrica XYZ es la adecuación, al contexto específico del tema y de la organización, de un Meta modelo de Planeamiento Estratégico. Lo esencial del proceso de Planeamiento Estratégico es el modelamiento y estructuración de lo que el método denomina Enunciados de Intención (sentido, propósito, finalidad) que los participantes e involucrados establecen para la organización. Se propone un método para formular el PETIC en la empresa de generación eléctrica XYZ para el período 2018-2021.",Maestro en Ciencias: Informática con mención en tecnologías de la información y comunicación en Gestión y Educación,2017
Universidad Nacional de San Agustín de Arequipa,"Diseño, desarrollo e implementación de un software para un mejor control genético y mejor comercialización de la fibra y de los animales de alpaca en la provincia de Caylloma, región de Arequipa 2016","Gamarra Oré, Julio César","Huertas Niquen, Percy Oscar","Fernández Baca manifiesta que, para el mejoramiento de la calidad genética de especies pecuarias como el vacuno, ovino y porcino, se puede recurrir muy fácilmente a la importación de reproductores de importantes centros de crianza con mayores niveles de producción, y de esta manera obtener animales mejorados para la producción y el mejoramiento por cruzamiento. En el caso de las alpacas, por ahora, no contamos con la posibilidad de aplicar esta estrategia, ya que no existen centros de crianza tecnificada en otros países del mundo que puedan garantizarnos la obtención de animales de alto valor genético como para ser utilizados en el mejoramiento genético de los rebaños de camélidos. Por lo tanto, la opción en la actualidad para poder elevar la calidad genética de las alpacas, y mejorar sus niveles de productividad, es mediante la puesta en marcha de un programa de Mejoramiento Genético acompañado de un software especializado. Esta acción permitirá identificar y seleccionar animales sobresalientes en su medio ambiente de manera más rápida y sencilla, y por tanto obtener reproductores de alta calidad genética, para ser usados en los rebaños comerciales. [1] El presente trabajo de investigación busca desarrollar un software que permite a los criadores de alpacas de la provincia de Caylloma obtener una mejora genética, lo cual contribuye a la sostenibilidad de las producciones e incrementar su rentabilidad, así mismo permitirá la comercialización de alpacas a través de las nuevas tecnologías, permitiendo mejorar su eficiencia y adaptarlas a la demanda divergente de los consumidores. Este software se transferirá a los pobladores de Caylloma que se dedican a esta actividad.",Ingeniero de Sistemas,2017
Universidad Nacional de San Agustín de Arequipa,"Implementación y desarrollo de sistemas automatizados para mejorar la programación y ejecución de procesos en la gestión municipal, basados en arquitectura Java ee - spring","Chambilla Quispe, Violeta Concepción", ,"El presente trabajo de suficiencia profesional está dividido en seis partes, la primera detalla mi hoja de vida, la segunda brinda información de la institución y mi participación en su estructura orgánica, la tercera describe el marco teórico, la cuarta el proyecto “implementación y desarrollo de sistemas automatizados para mejorar la programación y ejecución de procesos en la gestión Municipal, Basados En Arquitectura Java EE - Spring”. La quinta parte proporciona las conclusiones relacionadas al proyecto desarrollado, y las recomendaciones y finalmente la sexta parte lista la bibliografía base de este trabajo, y para culminar una serie de anexos.",Ingeniera de Sistemas,2017
Universidad Nacional de San Agustín de Arequipa,Transformación de lenguaje natural en Sparql para consultas de tipo Factoid,"Atencio Torres, Carlos Eduardo", ,"Durante los últimos años, grandes empresas de internet y equipos moviles han invertido en mejorar la interaccion hombre-computador a través de una interfaz en lenguaje natural. A estos dispositivos llamaremos AVI (Asistentes Virtuales Inteligentes) y ejemplo de estos tenemos a Google Now, Siri, Voice Mate, entre otros. Estos AVI en realidad son un sistema de Pregunta-Respuesta programado para atender las demandas del usuario en un determinado dominio o un determinado hardware. Tales sistemas poseen 3 modulos: (I) procesamiento de la consulta, (II) procesamiento de la informacion, y (III) procesamiento de la respuesta El presente trabajo se concentrar en el primer ítem y propondremos el uso de gramaticas y el patron Interpreter para transformar una consulta en lenguaje natural a un lenguaje formal, en este caso escogimos SPARQL, que es un lenguaje propuesto para trabajar en Web Semantica y se emplea para consultar ontologias.",Ingeniera de Sistemas,2017
Universidad Nacional de San Agustín de Arequipa,Un enfoque para la reparación de modelos tridimensionales con presencia de hoyos,"Romero Calla, Luciano Arnaldo","Lopez Del Alamo, Cristian Jose","Los modelos tridimensionales generalmente son representados como mallas triangulares, las cuales son obtenidas a través de dispositivos de detección 3D. Sin embargo, los modelos obtenidos presentan algunos defectos; siendo el más común, la presencia de hoyos, que, por lo general, se debe a la conclusión. La presencia de hoyos en mallas debe ser tratado como un paso de pre-procesamiento, para mejorar el desempeño de tareas subsecuentes en el área de Procesamiento Geométrico.El presente trabajo propone una técnica para el llenado automático de hoyos en mallas tridimensionales. Es importante que la topología de los nuevos parches sea coherente con la malla circundante. Además, la variación de curvatura entre la malla insertada y la malla existente debe ser mínima, de tal modo que los parches insertados no sean detectados fácilmente por un observador en el modelo reparado. Una forma de lograr este objetivo es aproximar una función que cubra suavemente los hoyos.",Licenciado en Ciencia de la Computación,2017
Universidad Nacional de San Agustín de Arequipa,Propuesta de un modelo para el reconocimiento de escenas violentas en video,"Ramírez Ticona, Jorge Thony","Rodríguez Gonzalez, Pedro Alex","Analizando que el problema de reconocimiento de acciones violentas se ha convertido en un tema de actualidad dentro de la visión computacional, la detección de peleas o comportamientos agresivos en el ámbito espacial han sido menos estudiadas en la literatura. Dicha capacidad puede ser muy útil en algunos escenarios de video vigilancia como en las cárceles, centros psiquiátricos, calles de la ciudad. Bajo este marco, las características espacio-temporales se extraen de las secuencias de video y se utilizan para la clasificación. A pesar de resultados alentadores en el que se alcanzaron cerca de 90 % de precisión para esta tarea específica. Se encontró que en los sistemas de video vigilancia actuales no se cuenta con una detección espacial la cual indica en que parte de la imagen sucede el evento violento. Para afrontar esta problemática actual, se realizó el trabajo de investigación el cual realiza un reconocimiento de escenas violentas en video, la misma que concluye que el uso de STIP y YOLO son necesarios para desarrollar una aplicación viable de reconocimiento espacial de violencia. Una importante contribución de esta tesis es proponer un método para el reconocimiento de violencia en video la misma se hace espacial y temporalmente. En esta tesis, se realiza una evaluación amplia utilizando más de 1000 videos; con el fin de dar a conocer las ventajas de usar descriptores locales para el reconocimiento de la violencia en video. De acuerdo con los experimentos, el método propuesto utiliza el descriptor STIP con una tasa de acierto de 90.4 % y una segmentación espacial de la violencia en el video con una tasa de acierto de 74 % las cuales produjeron mejores resultados que otros métodos de la literatura.",Ingeniero de Sistemas,2017
Universidad Nacional de San Agustín de Arequipa,Implementación de un sistema para controlar una silla de ruedas eléctrica utilizando redes neuronales y reconocimiento de patrones de voz,"Chalco Monroy, Gloria", ,"Por lo general una persona tetrapléjica aún puede hacer uso de su voz, a través de la cual solicita asistencia en sus actividades; es por ello que se propone el presente modelo ""implementación de un sistema para controlar una silla de ruedas eléctrica utilizando redes neuronales y reconocimiento de patrones de voz"", el cual pueda darle cierto nivel de independencia y mejor calidad de vida a las personas. La característica básica de este modelo consiste en el reconocimiento de órdenes vocales, las cuales serán analizadas por una red neuronal, éstas luego, son procesadas por un agente inteligente que analizará su estado, y además transmitirá esta orden a la interfaz de la silla de ruedas. Adicionalmente, y para validación del módulo se presentará un simulador donde se pueda apreciar, visualmente y en pantalla, las órdenes ejecutadas. Así tal modelo, intérprete de órdenes de voz pueda ser complementado por una interfaz electrónica, en un trabajo futuro, para ser aplicado en el control de una silla de ruedas eléctrica real.",Ingeniera de Sistemas,2017
Universidad Nacional de San Agustín de Arequipa,Detección de Key-Components en modelos deformables,"Fuentes Perez, Lizeth Joseline","Lopez Del Alamo, Cristian Jose","El análisis de modelos deformables tridimensionales tiene actualmente un creciente interés para muchos investigadores en el área de geometría computacional. El aumento de escáneres de rango y otros dispositivos que extraen información tridimensional, permite generar gran cantidad de modelos tridimensionales, los cuales tienen aplicaciones directas; por ejemplo: en la medicina, restauración de piezas de arte y nuevas interfaces interactivas, solo por mencionar algunas ´áreas de la industria. En la literatura existen técnicas orientadas a extraer características importantes, seleccionado solo algunos puntos importantes, los cuales serán denominados puntos de interés. Sin embargo, el nivel de granularidad con que es representado el modelo tridimensional mediante puntos de interés, es muy fino y requiere de mayor cantidad de información para representarlo.",Licenciado en Ciencia de la Computación,2017
Universidad Nacional de San Agustín de Arequipa,"Análisis, diseño e implementación de un software que determine la planificación de carga de un contenedor","Santos Ramos, Abraham Max", ,"La planificación de carga es una tarea importante en la logística de las empresas o instituciones. Usualmente en este problema se tiene contenedores con forma de paralelepípedo y la carga dispuesta en cajas y lo que se desea es encontrar una forma de colocar las cajas apiladas en un contenedor de manera que se aproveche al máximo el espacio del mismo. En ésta tesis se propone implementar un software de código abierto para la planificación de carga de un contenedor que tiene como parámetros las dimensiones (largo, ancho y alto) del contenedor y las dimensiones de cada una de las cajas, y que produce como resultado la posición de cada una de las cajas así como su orientación y el orden de su colocación dentro del contenedor. Para la implementación del software se ha hecho uso de un algoritmo de planificación de carga propuesto por el profesor David Pisinger en 1999 disponible en Internet. Se mencionan asimismo, en el estado del arte los problemas derivados del problema de planificación de carga y algunos de los algoritmos existentes. Este software propuesto permitirá optimizar el uso del espacio y así reducir costos en la logística del transporte y/o almacenamiento de carga.",Ingeniero de Sistemas,2017
Universidad Nacional de San Agustín de Arequipa,Detección en tiempo real de acciones violentas en secuencias de video con Vif y Horn-Schunck,"Machaca Arceda, Vicente Enrique","Gutierrez Caceres, Juan Carlos","Se presenta un método para la detección en tiempo real de acciones violentas en secuencias de video con Violent Flow (ViF), Horn-Schunck y una segmentación previa de los objetos en movimiento. Este proyecto es la continuación de la tesis de maestría [43] donde se concluyó que ViF con Horn-Schunck tenía un buen desempeño además de ser uno de los de menor tiempo de procesamiento en la literatura. Ahora se presenta otra mejora a ViF, en si se propone una segmentación a cada fotograma de una secuencia de video, esta consiste en considerar solo los objetos en movimiento de una escena, y solo en ella aplicar ViF con Horn-Schunck. Esta segmentación de los objetos en movimiento se logra substrayendo primero el fondo de la imagen con modelos mixtos Gaussianos, después se reduce el ruido con operadores morfológicos, y finalmente se obtiene un sector del fotograma conteniendo la mayor cantidad de objetos en movimiento. Los resultados fueron evaluados en las bases de datos de Películas, Hockey, Multitudes y una construida especialmente para este proyecto, llamada surveillance Violent Video (SVV). La segmentación de los objetos en movimiento previa a ViF, mejoró el acierto, llegando a valores de AUC de 1.0, 0.84, 0.875 y 0.83 para las bases de datos de Películas, Hockey, Multitudes y SVV respectivamente. También se evaluó cuanto le tomaba a ViF procesar un video de 2 segundos. ViF con IRLS (algoritmo original de flujo óptico) empleo 9.3594 segundos, mientras que con Horn-Schunck solo empleo 2.1563 segundos y al aplicar la segmentación de movimiento se incrementó el tiempo de procesamiento a 2.6563 segundos. Entonces utilizar ViF junto a Horn-Schunck resulta altamente aceptable debido a su bajo tiempo de procesamiento y al aplicar la segmentación de objetos en movimiento se mejoró el acierto aunque aumento un poco el tiempo de procesamiento, pero aun con esto la propuesta es una de las de menor tiempo en todo el estado del arte.",Ingeniero de Sistemas,2017
Universidad Nacional de San Agustín de Arequipa,Presentación y sustentación del informe memoria de experiencia profesional y rendimiento de una prueba de conocimientos - modalidad suficiencia profesional,"Zapana Chura, Johnny Rogger", ,"Las organizaciones optimizan sus procesos con soluciones tecnológicas con el propósito de hacer el trabajo más efectivo para sus usuarios. La tecnología para el desarrollo software avanza rápidamente, cada vez se ofrecen mejores técnicas y herramientas para la construcción de soluciones.Una consultora de tecnología de Información ofrece soluciones de Ingeniería de Software en el desarrollo de proyectos de software.En el presente informe se describe el desarrollo de dos proyectos de software: sistema de Seguimiento de procesos Judiciales, aplicación web desarrollada para la minera Xstrata, permite organizar los expedientes judiciales. Mobile Wallet, aplicación móvil desarrollada para el banco Wells Fargo, permite realizar compras en centros comerciales.",Ingeniero de Sistemas,2017
Universidad Nacional de San Agustín de Arequipa,Reconocimiento de granos de café verde arábiga sin defectos físicos en muestras usando visión artificial,"Ramirez Ticona, Juan","Rodríguez Gonzalez, Pedro Alex","El reconocimiento de granos de café verde arábiga sin defectos físicos es importante desde el punto de vista comercial, dado que el café de la especie arábiga es un producto de alta demanda en el mercado internacional. Dicho reconocimiento es propenso a errores, debido a que actualmente es realizado en su mayoría de forma manual y subjetiva. Los principales trabajos que tratan este problema basados en visión artificial requieren de prototipos de adquisición, que toman cada imagen desde un ángulo completamente vertical con respecto a la superficie que contiene a la muestra de granos de café. Cada uno de estos prototipos es un limitante para labores practicas debido a la dificultad de su implementación y al ángulo restrictivo. Por lo que el objetivo general de este trabajo es proponer un modelo para el reconocimiento automático de granos de café verde arábiga sin defectos físicos en imágenes de muestras capturadas manualmente con un ángulo de toma diagonal usando visión artificial y basándose para el reconocimiento en el estándar de la Specialty Coffee Association of America (SCAA) [1]. La metodología utilizada en este trabajo se divide en: adquisición de imágenes, mejora de imágenes, segmentación, extracción de características, clasificación y comparación con otros trabajos. Los resultados obtenidos en este trabajo muestran que el reconocedor puede ser utilizado en aplicaciones reales debido a que se alcanzó un overall accuracy del 99.60%.",Ingeniero de Sistemas,2017
Universidad Nacional de San Agustín de Arequipa,Reconocimiento de expresiones faciales invariante a la posición del rostro usando CLM,"Yari Ramos, Yessenia Deysi","Túpac Valdivia, Yván Jesús","Este trabajo de investigación se enfoca en el reconocimiento de expresiones faciales invariantes a la posición de la cabeza en el espacio tridimensional. Para poder realizar todo este proceso se ha trabajado con constrainedLocalModel-CLM, el cual nos ayuda a encontrar los puntos característicos en una imagen que contiene el rostro humano. La cantidad de puntos que se ha considerado en el modelo es de 68 que cubren todo el rostro. Después de encontrar los puntos en la imagen, se usa un modelo en 3D (CANDIDE 3), el cual consta de 113 vértices, los cuales solo serán usados algunos puntos (tanto con el modelo 3D como con los puntos encontrados) que nos ayudarán a encontrar la posición del rostro en el espacio tridimensional. Teniendo ya la posición podemos generar los modelos correspondientes para cada una de las expresiones faciales (7 en total, neutro, alegría, tristeza, temor, disgusto, sorpresa y repugnancia) y de esta forma calculando la menor distancia se puede obtener la expresión correcta. El porcentaje de error producido con esta técnica, es de 5.7 %, siendo comparable con otra técnicas que puede resolver el mismo problema.",Doctora en Ciencias de la Computación,2018
Universidad Nacional de San Agustín de Arequipa,Métodos semánticos para la recuperación de información en grandes volúmenes de datos: una Aquitectura Escalable y Eficiente,"Ocsa Mamani, Alexander Victor", ,"La creciente disponibilidad de datos en diferente dominio de aplicación ha motivado el desarrollo de técnicas de recuperación y descubrimiento de conocimiento en grandes volúmenes de datos. Recientes trabajos muestran que tanto las técnicas de aprendizaje profundo como nuevos métodos de búsqueda aproximada en dominio de datos complejos son campos de investigación importantes, donde tanto la eficiencia como la escalabilidad de los algoritmos son factores críticos. Para resolver el problema de escalabilidad se han propuesto muchos enfoques. En problemas de gran escala con datos en altas dimensiones, una solución de búsqueda aproximada con un análisis teórico solido se muestra más adecuado que una solución exacta con un modelo teórico débil. Algoritmos de búsqueda aproximada basados en hashing son propuestos para consultar en conjuntos de datos alta dimensiones debido a su velocidad de recuperación y bajo costo de almacenamiento. Por otro lado, en problemas donde se tiene grandes volúmenes de datos etiquetados las técnicas de aprendizaje profundo, como las redes convolucionales, se muestran más adecuadas conforme el número de ejemplos por clases crece.Estudios recientes, promueven el uso de la Red Neuronal Convolutiva (CNN) con técnicas de hashing para mejorar la precisión de la búsqueda de los k-vecinos más cercanos- KNN. Sin embargo, aun hay retos que resolver para encontrar una solución práctica y eficiente para indexar características CNN, tales como la necesidad de un proceso de entrenamiento intenso para lograr resultados de consulta precisos y la dependencia crítica de los parámetros. Con el fin de superar estos problemas, se propone un nuevo método de búsqueda por similitud, Deep frActal based Hashing (DAsH), para calcular los mejores valores de parámetros para una proyección óptima en un subespacio, explorando las correlaciones entre los atributos de las características CNN usando la teoría fractal. Además, inspirado por recientes avances en redes CNN, utilizamos no solo activaciones de capas inferiores que son más generales, sino también el conocimiento previo de los datos semánticos sobre la última capa CNN para mejorar la precisión de la búsqueda. Así, nuestro método produce una mejor representación del espacio de datos con un coste computacional menor para una mejor precisión. Esta mejora significativa en velocidad y precisión nos permite evaluar este esquema en conjuntos de datos reales y sintéticos.",Doctor en Ciencias de la Computación,2018
Universidad Nacional de San Agustín de Arequipa,Modelo hibrido de árbol de decisión difusa con optimización por enjambre de partículas para clasificación de Obesidad Escolar,"Sulla Torres, Jose Alfredo","Alfaro Casas, Luis","La incorporacion de inteligencia computacional en los diagnosticos en el campo de la salud es una tendencia nueva y con un gran número de aplicaciones médicas. Muchos de los procedimientos de diagnosticos médicos se pueden categorizar como tareas de clasicación de datos inteligentes (Fan et al., 2011). En este contexto se propone en esta investigacion un modelo híbrido que integra un arbol de decisión, lógica difusa, la optimización por enjambre de partículas (PSO, por sus siglas en ingles). Utilizando para ello el análisis de regresion paso a paso (SRA, por sus siglas en ingl es) para el preprocesamiento de los datos y determine el conjunto de variables independientes que mas cercanamente afecten a la variable dependiente (obesidad). Luego se utiliza un arbol de decisión difusa para la generación de las reglas optimizado con un enjambre de partículas para mejorar los resultados de la clasi caci on de obesidad escolar, se realiza la interpretabilidad y se compara los algoritmos multiobjetivos representativos con PSO para mostrar de interpretabilidad y precision.Los resultados muestran que el modelo propuesto por medio de un arbol de decisión difusa ofrece una forma comprensible del an alisis para problemas orientados a la clasi caci on con un grado de exactitud aceptable, en ese contexto, basado a los resultados obtenidos se obtuvo las reglas de decision que mostro como exactitud en la obesidad del sexo masculino 84 %, mientras que en el sexo femenino la exactitud fue 89 %. La ambigüedad obtenida de las reglas aprendidas en el sexo masculino fue 0.04 y en escolares del sexo femenino fue ligeramente inferior (0.01). La optimizaci on por medio del algoritmo PSO mostro despues de 500 iteraciones un error RMSE de 12.04 para escolares masculinos y de 11.20 para escolares femeninos. Se concluye que de acuerdo a los resultados calculados es posible aplicar el modelo propuesto para la clasi cacion de obesidad de un modo aceptable. Adem as, se valido el uso de algoritmos multiobjetivos MOPSO utilizando un conjunto de funciones de evaluación ZDT, WFD y DTLZ que demostraron que se obtienen mejores resultados.",Doctor en Ciencias de la Computación,2018
Universidad Nacional de San Agustín de Arequipa,"Modelo pedagógico inteligente con inmersión háptica, basado en el enfoque de aprendizaje de la Programación Neuro-Lingüística (PNL)","Choquehuayta Palomino, Simón Ángel","Alfaro Casas, Luis","En este trabajo de investigación se presenta un modelo inteligente de entorno de apren- dizaje basado en: Minería de datos (DM), interacción háptica y la Programación Neuro- Lingüística (PNL). El modelo toma en cuenta los estilos de aprendizaje del educando en función de ellos se definen las estrategias de enseñanza y se proponen las actividades de “coaching” en una plataforma interactiva con inmersión kinésico-háptico. El modelo es aplicable al dominio del área de psicomotricidad, y cuenta con estrategias de enseñanza que se adaptan al estilo de aprendizaje y fortalecen el canal predominante de aprediza- je. Este canal se extrae utilizando clustering FarthesFirst. El entregable final del modelo Tusuna-pad (Interfaz y dispositivo) técnicamente llamado EAIHPNL (Entorno de apren- dizaje de interacción háptica con PNL), fue implementado usando el motor de juegos Unity 5.5. Se validó el modelo en alumnos de Educación Básica Regular (EBR) de la Institución Educativa (IE) 40134, Arequipa-Perú. El entregable obtuvo un 94,18 % de aceptación en diseño computacional y diseño pedagógico. Asimismo se obtienen nuevos instrumentos de evaluación modificados y adaptados para estudios posteriores.",Doctor en Ciencias de la Computación,2018
Universidad Nacional de San Agustín de Arequipa,Planificación de Tareas Dependientes Representadas por Grafos Acíclicos Dirigidos para Sistemas Computacionales Heterogéneos,"Mamani Aliaga, Alvaro Henry","Alfaro Casas, Luis","En la actualidad, el procesamiento de grandes cantidades de datos es requerido por diferentes áreas, tanto en la academia como en la industria, para esto se utilizan sistemas computacionales heterogéneos, sin embargo, un aspecto muy importante en el procesamiento es la plani cación de tareas. En el presente trabajo, se propone una metodología para la selección de un adecuado algoritmo de plani cación, siguiendo métricas, las cuales brindan los datos necesarios para saber si el algoritmo tendrá buen desempeño tanto en el tipo de aplicaciones como en las características del sistema heterogéneo. Ademas, se proponen dos algoritmos de plani cación: (i) Predict Earliest Finish Time with Clustering - PEFTC, el cual toma en cuenta las mejores características del algoritmo PEFT, pero en la etapa de asignación de recursos lo realiza en agrupaciones de tareas, esto mejora el desempeño al minimizar la comunicación entre tareas; (ii) Constrained Predict Earliest Finish Time - CoPEFT, el cual fue basado en las mejores características de los algoritmos PEFT y CEFT, aprovecha el cálculo que se realiza en el algoritmo CEFT, al encontrar caminos críticos restringidos, y en la fase de asignación de recursos utiliza la tabla de costos optimista propuesto por el algoritmo PEFT, preservando la complejidad algorítmica establecida en ambos algoritmos.",Doctor en Ciencias de la Computación,2018
Universidad Nacional de San Agustín de Arequipa,Planificación de Tareas Dependientes Representadas por Grafos Acíclicos Dirigidos para Sistemas Computacionales Heterogéneos,"Mamani Aliaga, Alvaro Henry","Alfaro Casas, Luis","En la actualidad, el procesamiento de grandes cantidades de datos es requerido por diferentes áreas, tanto en la academia como en la industria, para esto se utilizan sistemas computacionales heterogéneos, sin embargo, un aspecto muy importante en el procesamiento es la planificación de tareas. En el presente trabajo, se propone una metodología para la selección de un adecuado algoritmo de planificación, siguiendo métricas, las cuales brindan los datos necesarios para saber si el algoritmo tendrá buen desempeño tanto en el tipo de aplicaciones como en las características del sistema heterogéneo. Ademas, se proponen dos algoritmos de plani cación: (i) Predict Earliest Finish Time with Clustering - PEFTC, el cual toma en cuenta las mejores características del algoritmo PEFT, pero en la etapa de asignación de recursos lo realiza en agrupaciones de tareas, esto mejora el desempeño al minimizar la comunicación entre tareas; (ii) Constrained Predict Earliest Finish Time - CoPEFT, el cual fue basado en las mejores características de los algoritmos PEFT y CEFT, aprovecha el cálculo que se realiza en el algoritmo CEFT, al encontrar caminos críticos restringidos, y en la fase de asignación de recursos utiliza la tabla de costos optimista propuesto por el algoritmo PEFT, preservando la complejidad algorítmica establecida en ambos algoritmos",Doctor en Ciencias de la Computación,2018
Universidad Nacional de San Agustín de Arequipa,"Análisis, diseño e implementación de un sistema de Software para la automatización de las actividades del área de congelado en una empresa Avícola peruana","Nieto Hinojosa, Luis Manuel", ,"El presente informe por experiencia profesional describe el desarrollo de un proyecto de software en una empresa avícola peruana, cuyo objetivo debe ser la automatización de las actividades que se realizan en el área de congelado; actividades como el ingreso de producto a los túneles de congelado y el armado de sacos y/o cajas de producto congelado. En la actualidad, no existe control alguno sobre el producto que se destina al túnel de congelado, no existe forma de obtener el número exacto ni de unidades ni de kilos, ni cuáles son los productos que ingresan, salen y permanecen en los túneles de congelado. No existe forma de saber exactamente, dónde se encuentra el producto que sale de planta, no se pueden obtener los tiempos de permanencia en túnel, los tiempos en pasadizo, ni los tiempos del producto que sale de túnel de congelado, ya sea en forma de sacos y/o cajas. Además, existen inconsistencias en los valores y configuraciones entre la planta, el área de congelado, despacho y el sistema de software actual. La configuración de los productos y sus rangos, no corresponde con la realidad, el proceso de etiquetado y la distribución de los productos no se realizan correctamente. En el área de congelado, el peso y el contenido de los sacos y/o cajas, depende del criterio del capataz de congelado, además se observan esfuerzos humanos innecesarios. Hasta ahora, hemos dado un panorama general, hay muchas detalles que se abordarán a profundidad más adelante, el objetivo principal de este proyecto es desarrollar un software que permita automatizar las actividades que se desarrollan en el área de congelado, el software debería permitir registrar el ingreso de producto a los túneles de congelado, debería permitir conocer que es lo que hay actualmente en cada túnel de congelado y debería permitir el registro de sacos y/o cajas transfiriendo automáticamente el saldo de los productos ensacados y/o encajados al área de Despacho. Otro de los objetivos del proyecto, es corregir todas las inconsistencias que se observan actualmente, en Planta, Congelado y Ventas.",Ingeniero de Sistemas,2018
Universidad Nacional de San Agustín de Arequipa,"Mejoramiento del proceso de detección de defectos visuales en castañas, con fines de exportación","Cervantes Jilaja, Claudia","Patiño Escarcina, Raquel Esperanza","Actualmente, una importante actividad económica en algunos países de América del sur como Pe- rú, Bolivia y Brasil es el comercio de frutos secos como castañas; sin embargo, antes de exportarlos es necesario ejecutar un proceso de control de calidad que implica el estado de madurez, detección e iden- tificación de defectos, daños o enfermedades; de manera que posteriormente realice una clasificación del producto. Todo ello se realiza teniendo en cuenta las características externas del producto mediante la obtención de descriptores de color, forma, tamaño y textura; estos descriptores se usan en la detección e identificación de los defectos de frutas u objetos. Esta tesis propone la automatización de la detección e identificación de defectos visuales en un objeto, donde se divide el objeto en dos regiones (una oscura y otra clara) debido a los colores similares que presentan los defectos con el objeto; para detectar la presencia de defectos se basa en la textura de cada región, se propone el algoritmo de Detectar Defecto (Alg. 2 y 3) que utiliza el descriptor de Primer Orden (Alg. 5) para identificar la región donde el defecto es encontrado. Posteriormente, los descriptores de color, tamaño y textura se utilizan para la identificación de los defectos específicos a través de la Segmentación por Color y Tamaño (Alg. 4) para identificar una variedad de colores oscuros y claros y el descriptor de Primer Orden para identificar texturas ásperas o rugosas; estos algoritmos dan la posición central y tamaño de los defectos encontrados. Esta propuesta fue implementado y probado en la base de datos para el proceso de detección de de- fectos visuales en castañas, permite mejorar e incrementar la calidad de las castañas para su comercio internacional. Los resultados experimentales (Imágenes sin Entrenamiento – 60 % de la Base de Datos) muestran que esta propuesta tiene una tasa de eficiencia de 97.90 % con un tiempo de procesamiento de 25 ms en el peor y 17 ms en el mejor de los casos por cada imagen procesada; estos resultados mejoran los resultados del algoritmo del proyecto [Proy.PIPEA_134, 2013] donde la tasa de eficiencia es 91.06 % con un tiempo de procesamiento de 43 ms.",Ingeniero de Sistemas,2018
Universidad Nacional de San Agustín de Arequipa,Algoritmos para generación de mallas cuadrilaterales por descomposición jerárquica de dominio,"Quispe Ccapacca, Edgar","Paz Valderrama, Alfredo","El método de elementos finitos (MEF), es una técnica numérica muy utilizada en la simulación por computadora de problemas físicos y de ingeniería. El MEF es aplicado sobre modelos geométricos que representan la forma real de los objetos. En el caso bidimensional, los objetos son representados mediante entidades geométricas tales como vértices, curvas y superficies. El modelo es creado trazando curvas (líneas, arcos, splines, etc.) que dividen el plano en regiones cerradas delimitadas por una o varias curvas, sobre las cuales se definen superficies que indican que tales regiones no deben ser consideradas huecas. En el ámbito de la generación de mallas, estas superficies son llamadas “dominios”. La aplicación del MEF requiere discretización de los dominios en pequeños polígonos de forma triangular o cuadrilateral. Este procedimiento es llamado “generación de mallas triangulares o cuadrilaterales” según corresponda. La generación de malla requiere que las curvas que delimitan los dominios hayan sido subdivididas en varios segmentos. El “método de interpolación transfinita” es muy utilizado para generar mallas cuadrilaterales sobre dominios delimitados por cuatro curvas, pero se limita a dominios en los cuales las curvas que son opuestas tienen igual número de subdivisiones. En este trabajo se describe un procedimiento llamado “descomposición jerárquica de dominio”, el cual resuelve este inconveniente descomponiendo el dominio en subdominios. Cada subdominio también puede ser descompuesto de forma independiente, hasta obtener subdominios donde sea posible aplicar el “método de interpolación transfinita”. La descomposición de dominios es realizada mediante la aplicación de patrones, según ciertas condiciones satisfechas por los dominios. Los patrones dan una representación de como un dominio debe ser descompuesto. Los ejemplos presentados sobre dominios con diferentes geometrías y distribución arbitraria de subdivisiones sobre las curvas, muestran la utilidad de la metodología descrita.",Ingeniero de Sistemas,2018
Universidad Nacional de San Agustín de Arequipa,Proyecto de elaboración de software para la gestión contable – empresa Mapla S.A.C.,"Alfaro Vizcarra, Irene Lucía", ,"En la actualidad las organizaciones se ven involucradas en un entorno tan competitivo y cambiante, por lo que se tiene la necesidad cada vez mas frecuente de acometer proyectos, ya sean como para respuesta a un mercado en potencia (proyectos externos), como también aquellos que están dirigidos a satisfacer necesidades propias (proyectos internos). Por otra parte, las características intrínsecas a cada proyecto, su planificación, programación y control hace necesario disponer de métodos y técnicas propias, puesto que en caso contrario la capacidad de generación de valor por parte de las empresas se vería fuertemente limitada. MAPLA opera y mantiene instalaciones complejas en todo el Perú, siendo así una compañía experimentada en operaciones y mantenimiento para la industria, metales, energía y manufactura. MAPLA ayuda a los propietarios de plantas e instalaciones a que los costos operativos y generales se reduzcan y la productividad y rendimiento de la planta aumente, así como también analiza el funcionamiento operativo de la planta y ayuda a identificar paso a paso los cambios y oportunidades de mejora. Ante la necesidad de mantener las cuentas claras y de esa manera administrar correctamente el insumo económico se construye un sistema de gestión contable con la necesidad de mantener concentrada esta información para una oportuna toma de decisiones.",Ingeniero de Sistemas,2018
Universidad Nacional de San Agustín de Arequipa,Sistema híbrido basado en el algoritmo de optimización de colonias de hormigas y la heurística 2-opt (swap/move) para la distribución de estados de cuenta,"Vargas Monje, Mauricio Raul", ,"En este trabajo de investigación se estudia un problema de diseño de rutas óptimas para lograr una cadena de abastecimiento integrando las decisiones de ruteo y localización. Se presenta un modelo matemático basado en la automatización de las colonias de hormigas para el problema de localización y ruteo, el cual considera un conjunto discreto de paradas candidatos con un costo de apertura fijo y capacidad determinada. Se considera una flota heterogénea con tamaño, capacidad limitada y costos de utilización diferenciados. Se asume una demanda determinística y un único periodo de planeación. Asimismo, se propone un método de solución de tipo metaheurístico, que toma como base una búsqueda local iterativa. Se realizan pruebas computacionales en un grupo de instancias tomadas, que permiten analizar la asertividad del método construido apoyándose en los tiempos y calidad de la solución obtenida, para lo cual se emplearon las fichas de observación documental bajo un método científico riguroso. Se busca elaborar un sistema híbrido para solucionar la distribución de estados de cuenta mediante el algoritmo de optimización de colonias de hormigas y la heurística 2-OPT. Esta metodología permite escoger la mejor ruta para lograr una distribución óptima, los resultados muestran rutas óptimas según los puntos de entrega. Como conclusión se obtiene que el sistema hibrido propuesto escoge la mejor ruta que le permite la distribución de estados de cuenta sin consumir tiempo extra; el procedimiento constituye una guía que permite identificar las insatisfacciones de los clientes proponiendo soluciones que permitieron alcanzar las metas trazadas.",Ingeniero de Sistemas,2018
Universidad Nacional de San Agustín de Arequipa,"Asistencia del diagnóstico de osteoporosis en el fémur proximal, mediante el análisis y evaluación de métodos de procesamiento de imágenes de Rayos X","Gallegos Guillen, Joel Oswaldo","Castro Gutierrez, Eveling Gloria","Las complicaciones traumatológicas asociadas al fémur proximal, tal como la osteoporosis constituye un problema clínico que origina aproximadamente el 25% de mortalidad; siendo las personas adultas y mujeres en etapa de menopausia las más propensas a contraerla. La incidencia de este tipo de complicaciones tiene implicancias tanto de forma inmediata como tardía produciendo cerca de 200 mil fracturas del fémur al año. El análisis de una imagen de rayos X es el primer paso para evaluar este tipo de complicaciones y determinar el grado de ésta enfermedad. La detección del patrón trabecular en el fémur proximal, sirve como segunda opinión para el médico especialista. La tesis presentada propone el análisis y evaluación de métodos de procesamiento de imágenes de Rayos X para la asistencia del diagnóstico de osteoporosis. Los métodos de procesamiento involucran etapas como: a) Preprocesamiento, b) Segmentación, b) Extracción de Características, y c) Clasificación; siendo etapas indispensables para la detección del patrón trabecular en imágenes de Rayos X. Para evaluar el desempeño de los métodos de preprocesamiento se utilizó la métrica de evaluación Peak Signal-to-Noise Ratio (PSNR) que permite medir la calidad de la imagen. Los resultados experimentales alcanzan un PSNR de 34.92 Db utilizando el Filtro Mediana. Asimismo, se utilizó un Gold-Standard de segmentación y de clasificación con la ayuda de un especialista en traumatología para evaluar y comparar los resultados obtenidos. Los resultados muestran un coeficiente de DICE del 83% en segmentación y un 87% de precisión en la clasificación de osteoporosis de fémur proximal. Finalmente, el tiempo computacional empleado es de 76 segundos por imagen de rayos X.",Ingeniero de Sistemas,2018
Universidad Nacional de San Agustín de Arequipa,Clasificación de señales mioeléctricas superficiales de los movimientos de la mano utilizando técnicas de aprendizaje supervisado,"Galarza Flores, Marisol Cristel","Paz Valderrama, Alfredo","La falta total o parcial de un miembro superior en una persona disminuye su capacidad para desarrolarse con normalidad en actividades cotidianas. Existen diversas herramientas que ayudan a las personas amputadas, entre estas herramientas las extremidades artificiales son las que brindan mejores resultados en el proceso de rehabilitación de los amputados. Este estudio forma parte de uno mayor que tiene como objetivo desarrollar una prótesis mioeléctrica. Para este estudio se tomaron señales de cuatro movimientos de la mano (flexión, extensión, abrir mano y cerrar mano) usando la normativa SENIAM con electrodos bipolares superficiales, se procesaron estas señales, se extrajo un vector característico y se usaron dos clasificadores supervisados: Support Vector Machcine (SVM) y Redes Neuronales para clasificar estas señales en el movimiento que le corresponde. Los vectores característicos con los que se hicieron las pruebas están conformados por: los coeficientes de aproximación (cA) de la transformada Wavelet de la señal, los cien com- ponentes principales (PCA) de la señal, como algunas características estadísticas propias de la señal. Se probó la clasificación con cuatro movimientos diferentes, tomados en dos canales, con los vectores característicos propuestos y se comparó entrenando los clasificadores con la data pura de la señal (raw data), comprobándose que la extracción de características mejora el porcentaje de asertividad de la clasificación hasta en un 2.25 % en los mejores casos de clasificación obtenidos, que son el 91.00 % de señales correctamente clasificadas.",Ingeniera de Sistemas,2018
Universidad Nacional de San Agustín de Arequipa,Diseño de una plataforma Interoperable Ros - Lego Mindstorms EV3,"Rosas Cuevas, Yessica","Herrera Quispe, Jose Alfredo","En los últimos años, la tecnología ha cambiado radicalmente la forma de aprender. El aprendizaje basado en proyectos promueve la utilización de proyectos auténticos y realistas, basados en un problema altamente motivador y envolvente, dónde el alumno aprende todo lo involucrado en el proyecto. La robótica educativa y la programación son algunas de las herramientas propuestas para este nuevo tipo de aprendizaje, en estos proyectos los estudiantes visualizan rápidamente los resultados de los conocimientos adquiridos. Luego, herramientas como LEGO Mindstorms proveen todo un conjunto de soluciones diseñadas para potenciar el aprendizaje de forma cuantiosa sin embargo, su principal limitación es que posee software propietario, por ende está cerrado su integración a plataformas abiertas como Linux, haciendo que sus costos de adquisición sean altos. En este contexto, se propone una plataforma para controlar un kit de robótica educativa como LEGO. Una nueva implementación para la interoperabilidad, y de esa forma ampliar su soporte a un sistema operativo open source como Linux. La realización de este trabajo se ha dado integrando diferentes paquetes, de la herramienta estándar ROS (sistema Operativo Robótico) con Blockly. Los bloques desarrollados se han orientado a adolescentes novatos en programación, donde aprenden a identificar la funcionalidad de cada bloque. Luego de creado el prototipo funcional con el kit educativo, se ha realizado pruebas exitosas en un entorno real con adolescentes entre 14 y 16 años. Los estudiantes lograron controlar y realizar pequeños programas sobre un prototipo robótico mediante la plataforma. La aplicación futura de esta nueva plataforma, permitirá la integración de los Kit de desarrollo LEGO Mindstroms a kits de Software libre, reduciendo el costo de licencias y ampliando la posibilidad de las instituciones para la adquisición de Kits abiertos que se integren a los ya existentes.",Ingeniero de Sistemas,2018
Universidad Nacional de San Agustín de Arequipa,Clasificación de modelos tridimensionales no rígidos mediante redes neuronales convolucionales y descriptores espectrales,"Llerena Quenaya, Jan Franco","Lopez Del Alamo, Cristian Jose","En años recientes, la comunidad científica ha mostrado un interés creciente hacia el análisis de modelos tridimensionales. Los avances en tecnología y la aparición de hardware específico para la captura y digitalización de información tridimensional ha hecho esto posible. En esta tesis, se propone un algoritmo para resolver el problema de clasificación en modelos tridimensionales no rígidos. En otras palabras, el objetivo es determinar la clase a la cual pertenece un objeto de entrada. Para ello, se desarrolló un algoritmo basado en la detección de puntos de interés, el uso de descriptores espectrales y técnicas de deep learning. La principal contribución de este trabajo es una técnica que permitirá ayudar en tareas de clasificación, y que muestra robustez ante ciertos escenarios, puesto que tiene en consideración las diferentes transformaciones y/o deformaciones a las que se ven sujetos los modelos. Los resultados son evaluados y analizados usando la base de datos de SHREC 2011 (Non-Rigid Classification Benchmark), la cual contiene diferentes clases de modelos. Más aún, la propuesta es comparada con algunos métodos recientes del estado del arte, mostrando resultados promisorios. Considerando dos particiones diferentes de la colección de modelos, se obtiene una precisión del 91 % y del 95 % para cada caso, valores que superan a otros enfoques. Asimismo, nuestros resultados muestran robustez ante transformaciones como isometrías, ruido Gaussiano, agujeros, micro agujeros, escalamiento, sampling y shotnoise.",Ingeniero de Sistemas,2018
Universidad Nacional de San Agustín de Arequipa,"Elección de un método de procesamiento de imágenes de rayos X, para el soporte del diagnóstico de lesiones traumatológicas de la estructura pélvica","Estacio Cerquin, Laura Jovani","Castro Gutierrez, Eveling Gloria","Las lesiones traumatológicas de la pelvis; concretamente fracturas de acetábulo, constituyen un problema clínico que afecta fundamentalmente a personas entre 21 a 30 años, siendo el 65% de sexo masculino. La incidencia de este tipo de lesiones se ha incrementado debido a accidentes de tránsito; teniendo implicancias tanto de forma inmediata como tardía. El análisis de una imagen de rayos X es el primer paso para evaluar este tipo de lesiones y determinar el tipo de tratamiento que debe seguir el paciente. El identificar si la lesión traumatológica localizada en la pelvis, es una fractura de acetábulo o no, sirve como soporte para todos aquellos cirujanos ortopédicos que deben lidiar con este tipo de lesiones. La tesis presentada propone elegir un método de procesamiento de imágenes de rayos X, para el soporte del diagnóstico de lesiones traumatológicas de la estructura pélvica, con el objetivo de servir como segunda opinión en el diagnóstico de fracturas de acetábulo para investigación básica o estudios de salud pública. La elección del método de procesamiento de imágenes de Rayos X involucra el desarrollo de tres etapas: a) Preprocesamiento, b) Segmentación - Registro, y c) Clasificación; siendo etapas indispensables para la detección de fracturas de acetábulo mediante el análisis de imágenes de rayos X. Para evaluar el desempeño de los métodos de preprocesamiento se utilizó la métrica de evaluación Peak Signal-to-Noise Ratio (PSNR) que permite medir la calidad de la imagen. Los resultados experimentales alcanzan un PSNR de 75.55 para un método de preprocesamiento combinado, el cual se denominó como CLAHE-GUIDED. Asimismo, se generó un Gold-Standard de segmentación y de clasificación con la ayuda de un especialista en traumatología para evaluar y comparar los resultados obtenidos. Los resultados alcanzaron un índice de solapamiento (DICE Coefficient) de 78% con respecto al Gold-Standard de Segmentación y un 80% de precisión en la clasificación de fracturas de acetábulo con respecto al Gold-Standard de Clasificación. Finalmente, el tiempo computacional empleado por el método de procesamiento de imágenes de rayos X elegido es equivalente a 2.35 minutos por imagen de rayos X.",Ingeniero de Sistemas,2018
Universidad Nacional de San Agustín de Arequipa,"Modelo estocástico basado en redes neuronales no tradicionales aplicada a la generación de caudales mensuales caso: Cuenca del rio Chili , Arequipa","Luque Mamani, Edson Francisco","Herrera Quispe, Jose Alfredo","Las investigaciones en recursos hídricos pueden involucrar la generación de datos y/o pronóstico no sólo de variables hidrológicas sino de otras variables derivadas que permitan reducir pérdidas de tipo económicas y sociales, dimensionando y escenificando el impacto de una sequía, inundación y principalmente la demanda poblacional. Por lo tanto, la búsqueda de un diseño óptimo en un proyecto de gestión del agua frecuentemente involucra encontrar un método o técnica que genere largas secuencias de las características de los flujos(caudales) en este caso de un río en cuestión. Estas secuencias consideradas como series temporales pueden ser usadas para analizar y optimizar el desempeño del proyecto diseñado. Con el fin de cubrir esos requerimientos, este trabajo tiene como objetivo la elaboración de un nuevo modelo de proceso estocástico para ser aplicado en problemas que envuelven fenómenos de comportamiento estocástico y de características periódicas en sus propiedades probabilísticas como media y varianza. Para esto fueron usados dos componentes, el primero, un tipo de red neuronal recurrente introducido en la literatura denominado Echo State Network(ESN), siendo el componente determinista. Una característica interesante de ESN es que a partir de ciertas propiedades algebraicas, entrenar solamente la capa de salida de la red es a menudo suficiente para alcanzar un desempeño excelente en aplicaciones prácticas. La segunda parte del modelo, es un componente aleatorio que incorpora al modelo la incertidumbre asociada a los procesos hidrológicos. El modelo finalmente es llamado MEESN. Este fue calibrado y validado en series temporales mensuales de cuatro cuencas hidrográficas de MOPEX, así como en el ámbito local en series de la cuenca del Chili. El nuevo modelo fue comparado con modelos presentes en la literatura como el modelo; PEN, Thomas & Fiering y ANFIS. Los resultados muestran que MEESN y su versión modificada MEESN+TSM (que considera una variable exógena) ofrecen una alternativa prometedora para propósitos de simulación, con potencial interesante en el contexto de los recursos hidrometeorológicos.",Ingeniero de Sistemas,2018
Universidad Nacional de San Agustín de Arequipa,Visualización interactiva de superficies implícitas inmersas en el espacio R3,"Gallegos Velgara, Henry Giovanny", ,"El principal objetivo de este trabajo es presentar un nuevo método para la visualización de superficies implícitas de dimensión 2 inmersas en el espacio ℝ3. Ese método consiste primeramente de un pré-procesamiento en CPU usando una estructura Octree, y usando la Aritmética Intervalar para encontrar las regiones de dominio donde está presente la superficie implícita. La información del Octree es enviada a la GPU usando la estructura de Bounding Volume Sphere, dicha información es procesada en la GPU para efectuar la visualización y para eso fue utilizado una generalización de la técnica Ray Tracing.",Ingeniero de Sistemas,2018
Universidad Nacional de San Agustín de Arequipa,Método computacional para el análisis de la duración y dispersión de la onda P en electrocardiogramas mediante el empleo de la Transformada Wavelet,"Condori Valeriano, Ervert","Huertas Niquen, Percy Oscar","El objetivo de este proyecto es determinar la duración y la dispersión de la onda P de los electrocardiogramas (ECG). Esta onda es importante para determinar las alteraciones potenciales de la fibrilación auricular, estos diagnósticos se realizan mediante inspección visual, lo cual es una tarea difícil debido a ser una onda pequeña y mezclada con ruido, el uso de la transformada Wavelet usando el análisis multirresolución en campos locales para identificar mayores valores de dispersión de la onda P. Este modelo fue evaluado con una base de datos MIT-BIH con ritmo sinusal normal del segundo canal y la base de datos PTB con pacientes de control sanos de derivación II donde la diferencia entre su duración máxima y mínima de la onda P.",Ingeniero de Sistemas,2018
Universidad Nacional de San Agustín de Arequipa,Framework para diseño de contenidos en aplicaciones móviles de realidad aumentada orientado a personal no-experto,"Apaza Yllachura, Yuliana Guadalupe","Paz Valderrama, Alfredo","La realidad aumentada es una de las tecnologías que está teniendo un alto impacto en diferentes áreas como la educación, mantenimiento o desarrollo de videojuegos, medicina, arquitectura, etc. Una de sus principales desventajas, es la necesidad de que los creadores de contenidos tengan que tener ciertos conocimientos de programación. Una solución a este problema es el uso de frameworks de diseño de contenidos en alto nivel de realidad aumentada para facilitar el desarrollo de dichas aplicaciones. El presente trabajo propone un nuevo framework de diseño de contenidos en alto nivel de realidad aumentada para personal con conocimientos básicos o nulos de programación, el framework propuesto resume la experiencia recolectada de un estudio sistemático hecho sobre frameworks similares. Palabras clave: Realidad aumentada, aplicaciones móviles, herramienta de creación de aplicaciones de realidad aumentada.",Ingeniera de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Técnica para identificar conflictos entre los requerimientos no funcionales empleando casos de uso y matrices de cambios de cobertura,"Florez Mayorga, Yvan Rudy","Huertas Niquen, Percy Oscar","El presente trabajo de investigación consiste en proponer una técnica que permita identificar conflictos entre requerimientos no funcionales por medio de la utilización de casos de uso y matrices de cambios de cobertura. La idea de lograr estas identificaciones permitirá minimizar errores en la gestión de requerimientos no funcionales que tienen efecto sobre los requerimientos funcionales. El propósito es obtener un conjunto de conflictos en los requerimientos no funcionales, conflictos que ayudarán a una mejor gestión de los requerimientos en la construcción del producto de software, permitiendo de esta forma eliminar inconsistencias o ambigüedades en los requerimientos. Este problema es atacado en etapas tempranas de la construcción de software presentando una mayor orientación hacia los requerimientos no funcionales y su mal entendimiento. Para cumplir el objetivo se manipularon los requerimientos no funcionales por medio del empleo de plantillas de casos de uso modificados y la matriz de cambio de cobertura; artefactos que permiten detectar los conflictos surgidos entre requerimientos no funcionales. Asimismo, la técnica propuesta está constituida por un conjunto de pasos que conducen a comprender el impacto de los conflictos sobre los requerimientos funcionales.",Ingeniero de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Generación de un modelo 3D en base a imágenes de rayos X y una plantilla volumétrica para el planeamiento preoperatorio de reemplazo total articular de cadera,"Suca Velando, Christian Anthony","Castro Gutierrez, Eveling Gloria","Es indispensable desarrollar herramientas que permitan obtener resultados precisos en cuanto al diagnóstico de complicaciones traumatológicas haciendo uso de imágenes de rayos X, las cuales se consideran como primera fuente de información para el diagnóstico de complicaciones traumatológicas debido principalmente a su fácil adquisición y bajo costo. De este modo, la reconstrucción de un modelo 3D a partir de estas imágenes puede ser la solución para obtener resultados más precisos que una MRI o CT haciendo uso de métodos de visión computacional, tales como segmentación y registro. Finalmente, la generación de un modelo 3D de una estructura ósea determinada dará lugar al diseño de una prótesis específica del paciente; lo que facilita el proceso de decisión clínica para evaluaciones preoperatorias, evaluaciones postoperatorias e intervenciones quirúrgicas",Ingeniero de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Evaluación de usabilidad de dos aplicaciones de realidad aumentada,"Payalich Quispe, Claudia Paola","Corrales Delgado, Carlo José Luis","Las aplicaciones educativas buscan generar una contribución especifica en el aprendiza-je, ofreciendo metodologías y recursos de aprendizaje mas difíciles de entender a través de otros medios, como haciendo uso de tecnologías emergentes como la realidad aumentada (RA). La forma de interacción de los usuarios con sistemas que hacen uso de RA combina-da con técnicas de gamificacion, son diferentes a las tradicionales, actualmente existe gran cantidad de trabajos de investigación sobre evaluaciones de interfaz de usuario tradiciona-les, sin embargo aun no hay estándares que evalúen la usabilidad de un sistema que usa RA. En este trabajo se ha realizado una evaluación de usabilidad a aplicaciones basadas en RA. De los métodos de evaluación existentes, se eligieron dos métodos de inspección (recorrido cognitivo y evaluación heurística), uno de pruebas de usabilidad (evaluación de laboratorio) y uno de informes de usuario (Cuestionario SUS) asociados en dos grupos: Evaluación por Heurísticas y según la definición de usabilidad de la ISO 9241. Además se presenta una propuesta de heurísticas de evaluación de usabilidad de una aplicación en RA, tomando en cuenta las características propias de esta tecnología emer-gente [1]. Para validar la propuesta se utilizo la metodología planteada por Rusu. En ambos casos de evaluación, se identifico claramente los problemas clave en el diseño de la aplicación. Finalmente podemos afirmar que la propuesta de heurísticas planteada brinda una mejora de evaluación de un 31 % respecto a las heurísticas tradicionales que presentan ciertas limitaciones cuando son aplicadas a un dominio especifico como en RA. Así también la evaluación realizada según la ISO 9241, refleja similitud de resultados con la evaluación realizada por las heurísticas.",Ingeniera de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Modelo Inteligente para la Gestión de Aprendizaje aplicando Case Based Reasoning (CBR) y Reinforcement Learning (RL),"Flores Garcia, Anibal Fernando","Alfaro Casas, Luis","El presente trabajo de investigación presenta un nuevo modelo para la implementación de e-learning personalizado. El modelo propuesto considera el nivel de habilidades o conocimiento que un estudiante tiene en un tema en particular; esto se determina a través de una prueba de entrada (pretest); este aspecto es muy importante para evitar problemas conocidos como la ansiedad o el aburrimiento de acuerdo a la teoría de flujo. Adicionalmente, para determinar la secuencia óptima de recursos de aprendizaje para un estudiante, se trabajó de manera complementaria con dos técnicas de inteligencia artificial: El Razonamiento Basado en Casos (CBR) y el Aprendizaje por Refuerzo (Q-Learning). El Razonamiento Basado en Casos, permitió en base a casos de éxito del pasado, determinar la secuencia de recursos de aprendizaje más apropiada para el estudiante; y en caso de no haber casos muy similares, se eligió una secuencia de recursos de aprendizaje del conjunto de secuencias óptimas propuestas por el Aprendizaje con Refuerzo (Q-Learning).",Doctor en Ciencias de la Computación,2019
Universidad Nacional de San Agustín de Arequipa,Automatic Cyberbullying detection in Spanish - Language Social Networks using Sentiment Analysis Techniques,"Montufar Mercado, Rolfy Nixon","Castro Gutierrez, Eveling Gloria","El acoso cibernético es un problema creciente en nuestra sociedad que puede traer consecuencias fatales y puede presentarse en textos digitales, por ejemplo, en las redes sociales en línea. Hoy en día hay una gran variedad de trabajos centrados en la detección de acoso cibernético en textos digitales en el idioma inglés, sin embargo, en el idioma español hay pocos estudios que abordan este tema. Este trabajo tiene como objetivo detectar este acoso cibernético en las redes sociales, en idioma español. Se utilizaron técnicas de análisis de sentimientos, como bolsa de palabras, eliminación de signos y números, tokenización y derivación, así como un clasificador bayesiano. Los datos utilizados para el entrenamiento del clasificador bayesiano se obtuvieron del Diccionario Español de Afectos en el Lenguaje (SDAL), que es una base de datos formada por más de 2500 palabras evaluadas manualmente en tres dimensiones afectivas: agradabilidad, activación (pasivo, activo) y facilidad de imaginar, además se obtuvieron 595 palabras siguiendo el mismo procedimiento de SDAL, las cuales se recolectaron con la ayuda de los miembros del Centro de Investigación, Transferencia de tecnología y Desarrollo de Software (CiTeSoft). Como resultado, el software desarrollado tiene un 93% de éxito en las pruebas de validación realizadas.",Ingeniero de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Seguimiento visual de autos en videos grabados con drones,"Sumari Huayta, Felix Oliver","Gutierrez Caceres, Juan Carlos","En la actualidad el Seguimiento Visual de Objetos es utilizado en varias aplicaciones como, video vigilancia, rastreo de animales, control de tráfico, etc. El mundo ha sido testigo del surgimiento de un nuevo dispositivo, como es el vehículo aéreo no tripulado conocido como Drone. El área de Aprendizaje Profundo(CNNs) es muy popular en estos tiempos y ha sido el que mejores resultados ha obtenido en el seguimiento de objetos a pesar de ser lento en procesamiento. MDNet es un algoritmo basado en CNN y Aprendizaje Multidominion que ha obtenido mejores resultados en la evaluación preliminar realizada. En el presente trabajo se creo un DataSet propio denominado VATD, que a diferencia de otros, contienen videos de autos grabados con drones. Añadimos modelos de movimiento en el método MDNet y otros tres para compararlos. Los resultados obtenidos nos muestran que un modelo de movimiento puede aumentar el desempeño en los diferentes algoritmos, específicamente en MDNet se puede notar el aumento en el DataSet propuesto.",Licenciado en Ciencia de la Computación,2019
Universidad Nacional de San Agustín de Arequipa,Desarrollo de una plataforma en android para teleoperación de un sistema robótico en seguridad de empresas y almacenes,"Lovon Ramos, Percy Wilianson","Herrera Quispe, Jose Alfredo","En esta Tesis, se propone una plataforma para una teleoperación remota de un robot móvil aplicada a la detección de posibles intrusos en ambientes estructurados, para esta aplicación el ambiente será el almacén de una empresa. La plataforma propuesta será compatible con cualquier dispositivo móvil Android, contiene también un módulo de modelado y control de velocidad del robot móvil utilizado, en el cual asegura que los comandos enviados a través de la interfaz móvil se ejecuten de manera correcta. Además, se propone una fusión sensorial para mejorar la presentación de los datos de los sensores, de este modo los datos serán más entendibles al usuario final u operador. Para realizar la verificación de la plataforma propuesta, se realizaron dos tipos de experimentos, una en la cual se realiza pruebas al modelado y control del robot en una tarea de vigilancia autónoma usando un seguidor de línea. Para verificar la fusión sensorial se realizan pruebas de navegación semiautónoma usando la fusión sensorial para guiar al operador. La integración de hardware y software es realizada con el Robot Operating System (ROS), la comunicación de los diferentes elementos en la presente investigación es realizada con la biblioteca Sockets para Java la cual nos permitió utilizarla en Java y android Studio. Para la verificación de nuestra propuesta se utilizó un Kit Robotico Gopigo 3 y un Robot móvil de Vigilancia con locomoción de orugas.",Ingeniero de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Detección de armas de fuego utilizando redes convolucionales profundas,"Huaman Cruz, Jose Miguel","Gutierrez Caceres, Juan Carlos","Debido a la creciente inseguridad ciudadana que reporta el INEI en nuestro país, es que la mayoría de municipalidades ha optado por adquirir cámaras las cuales están conectadas a una central de monitoreo. Esto provoca que, el número de cámaras esté relacionado proporcionalmente con el número de personas que monitorea las mismas, es en este problema donde se puede proponer una solución. Uno de los modelos más usados son las Redes Neuronales Convolucionales, estas reciben una gran cantidad de imágenes sin pre procesar. Cada imagen tiene una anotación de donde se encuentra el objeto a reconocer, después de entrenar la red neuronal se valida si un objeto reconocido dentro de la imagen pertenece o no a la clase que se quiere aprender a reconocer. Para la presente investigación, se revisó varios artículos en el estado de arte y se observó que dos de ellos obtuvieron resultados iniciales en la detección de armas utilizando la red Faster R-CNN. Pero el tiempo de detección de armas después de entrenar fue de 7 frames per second(fps), existen nuevos modelos de redes convolucionales como You Only look Once (YOLO) que procesan imágenes alrededor de 45 fps por lo que se cree que se obtendría mejores resultados y se aproximaría a detección en tiempo real. Los resultados finales demostraron que si es posible detectar armas utilizando YOLO con una tasa de acierto del 97.30%. Cuando se realiza la prueba de un vídeo en vivo donde una persona saca un arma de su ropa, la red YOLO logra detectar el arma cuando se visualiza parcialmente el arma en el vídeo.",Licenciado en Ciencia de la Computación,2019
Universidad Nacional de San Agustín de Arequipa,Sistema de Evaluación de la calidad de servicios Web a través de la mineración de la experiencia de los usuarios,"Zela Ruiz, Jael Louis","Castro Gutierrez, Eveling Gloria","En los recientes años, la selección de servicios Web ha llegado a convertirse en una tarea no trivial, debido a la gran cantidad de servicios Web disponibles en la Internet y a los servicios Web funcionalmente similares. Consecuentemente, la calidad de Servicio (QoS) ha sido usado con el objetivo de comparar servicios similares en funcionalidad, donde valores de calidad son medidos usando diferentes atributos de calidad, tales como disponibilidad, tiempo de respuesta y rendimiento. Sin embargo, estos valores no siempre reflejan la calidad que los usuarios perciben. Hoy, con la Web 2.0, algunos directorios de servicios Web permiten a los usuarios publicar comentarios sobre los servicios indexados. Estos comentarios son frecuentemente analizados por potenciales usuarios, nuevo usuarios en busca de servicios, con el fin de tener un mayor conocimiento sobre la calidad de los servicios. Los comentarios de usuarios son un buen indicador debido que describen la calidad subjetiva de los usuarios, de los cuales se puede inferir y/o calcular una reputación del servicio Web. Por otro lado, debido al gran número de servicios y a la enorme cantidad de comentarios por servicio, la tarea de analizar comentarios llega a demandar bastante tiempo y ser tediosa para nuevo usuarios. Ante esta problemática, este trabajo propone un proceso de evaluación de la reputación de servicio Web basado en técnicas de Mineración de Textos. Este proceso recolecta y procesa los comentarios de usuarios para evaluar si el comentario presenta información subjetiva, así mismo evalúa la polaridad del comentario (positivo o negativo), esto refleja la satisfacción o decepción de los usuarios con respecto a la calidad del servicio Web. Para lograr este objetivo, se colectaron comentarios de dos directorios de servicios Web, ProgrammableWeb y G2Crowd, posteriormente varios tipos de características fueron extraídos de los comentarios como Bag of Words, Bigrams, Part of Speech y TF-IDF, así también diferentes algoritmos de aprendizaje de máquina fueron usados, tales como Naive Bayes, Support Vector Machine y Maximum Entropy. Finalmente, el Promedio Bayesiano fue usado para calcular un valor de reputación para los servicios. Una plataforma de consulta fue implementada para disponibilizarla al público en general para poder comparar y seleccionar servicios Web",Ingeniero de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Construcción de una herramienta para el análisis de requisitos de software descritos en lenguaje natural,"Sarmiento Calisaya, Edgar","Cornejo Aparicio, Victor Manuel","El análisis de requisitos de software en lenguaje natural se realiza generalmente a través de un procedimiento de lectura de documentos, búsqueda de defectos y corrección de problemas (revisiones); por lo tanto, el análisis es un proceso costoso, que requiere un gran esfuerzo, lleva mucho tiempo y es propenso a errores. Un enfoque automatizado que detecte ciertos tipos de defectos podría permitir un rápido análisis de requisitos. Este trabajo propone el uso de técnicas de Procesamiento del Lenguaje Natural y Redes de Petri como una manera eficaz de analizar los requisitos descritos utilizando lenguaje natural y escenarios. Este enfoque toma como entrada escenarios y genera un reporte de análisis como resultado. Para facilitar el análisis automatizado, los escenarios se verifican lingüísticamente y transforman en Redes de Petri. Evaluamos los aspectos estructurales y comportamentales de los escenarios y sus redes de Petri resultantes mediante la búsqueda de indicadores que proporcionen evidencia de violación (defectos) de las propiedades de no-ambigüedad, completitud, consistencia y corrección. La viabilidad de la propuesta se demuestra a través de cuatro estudios de caso, los cuales comparan los resultados obtenidos por nuestro enfoque con respecto a soluciones de referencia.",Ingeniero de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Sistema deductivo basado en grafos para la lógica intuicionista,"Quispe Cruz, Marcela","Gutierrez Caceres, Juan Carlos","El sistema geométrico de deducción denominado de N-Grafos fue introducido por De Oliveira en el año 2001. Las pruebas en este sistema son representadas por medio de digrafos. Estos grafos de pruebas se basan en la deducción natural y en el cálculo de secuentes de Gentzen, combinando ideas de cuatro abordajes geométricos consolidadas en la literatura de teoría de la prueba: las tablas de de-senvolvimiento (Kneale, 1957), redes-de-prueba (Girard, 1987), logical flow graphs (Buss, 1991), y principalmente pruebas-como-grafos (Statman, 1974). Dado que to-dos estos sistemas geométricos apelan a la simetría clásica entre las premisas y las conclusiones, proporcionar una versión intuicionista de cualquiera de estos es un ejercicio interesante para ampliar el rango de aplicabilidad del sistema geométrico en cuestión. En esta tesis se produce una versión intuicionista para los N-Grafos, basado en el sistema FIL de De Paiva y Pereira. Recuerde que FIL tiene conclusiones múltiples y utiliza un dispositivo de indexación en el secuente, que permite el seguimiento de las relaciones de dependencia entre fórmulas en el antecedente y consecuente del secuente. Una condición en la regla de implicación a la derecha asegura que sólo fórmulas constructivas válidas se deriven. Se demuestra la correctitud y completitud de estos N-Grafos intuicionistas con respecto a FIL",Ingeniera de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Deep neural network approaches for spanish sentiment analysis of short texts,"Ari Mamani, Disraeli Fausto","Ochoa Luna, José Eduardo","El análisis de sentimientos se ha investigado ampliamente en los últimos años. Si bien se han obtenido importantes resultados teóricos y prácticos, todavía hay margen de mejora. En particular, cuando se considera oraciones cortas y lenguas de bajos recursos. Por lo tanto, en este trabajo nos centramos en el análisis de sentimientos para los mensajes de Twitter en español. Exploramos la combinación de varias representaciones de palabras (Word2Vec, Glove, Fas-text) y modelos de redes neuronales profundas para clasificar textos cortos. Los enfoques anteriores de Deep Learning no pudieron obtener resultados óptimos para la clasificación de frases en español en Twitter. Por el contrario, mostramos resultados prometedores en esa dirección. Nuestra mejor configuración combina aumento de datos, representaciones de inserción de tres palabras, redes neuronales convolucionales y redes neuronales recurrentes. Esta configuración nos permite obtener resultados de vanguardia en el conjunto de datos de referencia español TASS/SEPLN, en términos de precisión.",Ingeniero de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Optimización del algoritmo Blast en el alineamiento de secuencias de ADN basado en procesamiento masivamente paralelo y distribuido,"Cruz Gamero, Franklin Luis Antonio","Gutierrez Caceres, Juan Carlos","En Bioinformática intentan definir modelos matemáticas de sistemas biológicos usando grandes cantidades de Unidades de Procesamiento Centra- les (CPUs), generando aplicaciones poco prácticas. Esto esta´ siendo optimizado por paralelismo usando unidad de Procesamiento Gráficos (GPU) y sistemas distribuidos. En este trabajo se presenta dos algoritmos de optimización del al- goritmo Basic Local Alignment Search Tool (BLAST), basado en tablas Hash, para el alineamiento de una secuencia (unisecuencial) y para el alineamiento de múltiples secuencias (multisecuencial) de consulta de Ácido Desoxirribonuclei- co (ADN), usando técnicas masivamente paralelas y distribuidas, mediante el modelo de programación con Compute Unified Device Architecture (CUDA) y uso de la GPU. Comparando su rendimiento en implementaciones secuenciales usando CPUs e implementaciones con la GPU. Evaluando su rendimiento en tiempo de procesamiento usando secuencias de ADN de referencia obtenidas de las bases de datos públicas National Center for Biotechnology Informa- tion (NCBI) y EMSEMBL como el genoma humano, mostrando los mejores rendimientos los algoritmos Cuda Naive (CN) para BLAST unisecuencial con un speedup de latencia de 1.24X sobre el algoritmo Knut Morris Pratt (KMP) y Cuda Base 5 (CB5) para BLAST multisecuencial con un speedup de 1.23X sobre el algoritmo Base 5 (B5), ambos con arquitectura GPU (con paralelis- mo) mejorando en el tiempo de procesamiento la heurística del BLAST.",Licenciado en Ciencia de la Computación,2019
Universidad Nacional de San Agustín de Arequipa,Desarrollo de un sistema integral de gestión y operaciones utilizando tecnologías web en la empresa de conductores Rutas Andinas S.A.C.,"Rodriguez Velasquez, Alexander Victor", ,"Desde el año 2013, la empresa “Escuela Profesional de Conductores RUTAS ANDINAS S.A.C.” decidió apostar por la innovación tecnológica, con el propósito de mejorar los procesos manuales de atención al cliente, almacenamiento de información y comunicación entre áreas, elaborando un sistema con énfasis administrativo y usando metodologías ágiles como SCRUM, aplicando una arquitectura ADR; utilizando el lenguaje PHP y un gestor de datos MYSQL, logrando un incremento considerable en la producción de la empresa. La premisa del desarrollo es contar con una tecnología de información, de ser posible gratuita, que permita el crecimiento rápido, así como una arquitectura segura con manejo de información a distancia. De igual forma era necesario contar con la flexibilidad, para el incremento de funcionalidades y el trabajo con posibles sucursales. En este documento presento las etapas de desarrollo, herramientas utilizadas, los retos y los logros obtenidos con la implementación de este sistema.",Ingeniero de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,"Reconocimiento de eventos anómalos en videos obtenidos de cámaras de vigilancia, usando redes convolucionales","Machaca Arcana, Luigy Alex","Gutierrez Caceres, Juan Carlos","El campo de reconocimiento de movimiento y acciones sobre vídeo vigilancia es un campo activo de computer vision(CV), hasta el momento ha recibido una especial atención debido a sus numerosas áreas de aplicación. De todas las técnicas aplicables a este campo, los modelos basados en deep learning se han revelado como un método capaz de afrontar la incertidumbre asociada a las acciones humanas. Algoritmos rápidos, precisos en la detección de objetos permiten a las compu- tadoras: conducir automóviles sin sensores especializados, permiten que los dispo- sitivos de teleasistencia, transmitir información de las escenas en tiempo real a los usuarios. Los sistemas de detección actuales han redefinido su propósito al momento de realizar una detección de una imagen. Técnicas como You Only Look Once (YOLO), region proposals+CNN (R- CNN), Single Shot Detector (SSD), entre otras. Nos presentan un nuevo enfoque de detección de objetos en una imagen. Razón por la cual describiremos y usaremos algoritmos basados en redes convolucionales que pueden detectar a las personas en la escena y analizar diferentes acciones además de sus patrones de movimiento, todo esto en tiempo real. La motivación para este trabajo es crear un sistema capaz de detectar eventos anómalos en vídeos de vigilancia, sin la necesidad de asistencia o intervención humana para la detección de dichos actos. En esta tesis, pondremos en contexto los principales tipos de modelos basados en deep learning, en al ámbito de detección de objetos y reconocimiento de acciones humanas, examinando sus ventajas y desventajas frente a las técnicas tradicionales y modelos actuales",Licenciado en Ciencia de la Computación,2019
Universidad Nacional de San Agustín de Arequipa,Medición de distancia articular de cadera usando imágenes de Rayos X para la asistencia en el diagnóstico de osteoartritis,"Gutiérrez Cervantes, Giancarlos","Castro Gutierrez, Eveling Gloria","La Osteoartritis de cadera, comúnmente conocida como artrosis de cadera, es una enfermedad articular degenerativa, la cual, afecta a personas de 50 años a más, especialmente a mujeres. Esta enfermedad es complicada de diagnosticar de manera objetiva, por lo que puede variar según el médico y muchas veces depende en gran medida de la experiencia del mismo. Para el diagnóstico de esta enfermedad generalmente se recurre a las imágenes de rayos X de cadera en vista Antero-posterior, para realizar mediciones de ciertos rasgos característicos de la enfermedad. Principalmente se realiza la medición de la distancia articular existente entre la cabeza femoral y la copa acetabular, donde la reducción de dicha distancia es un indicador claro de la presencia de osteoartritis. La medición de la distancia articular de cadera, implica la segmentación tanto del fémur como de la pelvis en las imágenes de rayos X, sin embargo, esta es una tarea desafiante debido a las variaciones de contraste existente en las imágenes de rayos X, a su vez, debido a factores externos, como las diferencias anatómicas entre los pacientes y la variación de las posturas de los mismos al obtener dichas imágenes radiográficas. El presente trabajo de tesis da a conocer un método de aprendizaje supervisado con un enfoque multiescala para realizar la segmentación de diversas estructuras óseas. Este método se basa en la detección de puntos de referencia, generalmente conocidos como Landmarks, a través de la estimación conjunta basada en los datos, gracias a la creación de un Gold Standard, mediante la segmentación manual realizada por un médico especialista haciendo uso de landmarks a lo largo de los contornos de las estructuras óseas, también se realiza un proceso de refinamiento para mejorar la precisión de la segmentación. Los landmarks detectados se usan para determinar la distancia articular a lo largo de la articulación de la cadera. La evaluación de la segmentación se realiza usando el Coeficiente de DICE, donde se obtuvo una precisión mayor al 88%. Para la medición del espacio articular se emplearon métricas, las cuales presentan una tasa de error aceptable. También se ha considerado y discutido brevemente el impacto del método propuesto para fines del diagnóstico de artrosis de cadera.",Ingeniero de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Health Care Reminder: Una Aplicación móvil adaptativa y persuasiva,"Suni Lopez, Franci","Condori Fernandez, Nelly","Actualmente, hay muchas aplicaciones que tienen como objetivo mejorar la adherencia terapéutica (por ejemplo, sensores ingeribles). Sin embargo, todavía existen limitaciones y desafíos para hacer que estas aplicaciones sean más adaptables a las necesidades reales de los usuarios. Además, actualmente hay muchos dispositivos(sensores) para colectar datos fisiológicos (e.g., Moodmetric), por lo tanto, los servicios de atención medica pueden explotar las señales biométricas para comprender mejor las necesidades de los usuarios. En esta tesis se propone mejorar la adherencia terapéutica en personas mayores con diagnóstico de hipertensión. Para hacer esto, se propone una aplicación móvil adaptativa y persuasiva (llamada Health Care Remin- der ), la cual reconoce las emociones del usuario (i.e., estrés) a partir de datos fisiológicos (actividad electrodermal). Con esta información, la aplicación envía mensajes/recomendaciones de diferentes niveles de persuasión. Los datos fisiológicos son recolectados por el E4-wristband, un dispositivo inalámbrico con múltiples sensores. Se han desarrollado dos fases de experimentos con usuarios reales para evaluar el correcto desempeño de aplicación móvil propuesta y sus respectivas funcionalidades. Los resultados obtenidos indican que el motor de inferencia del Health Care Reminder son coherentes con los valores esperados. Además, se ha corroborado que el uso de la aplicación móvil y el dispositivo E4-wristband no causan alguna incomodidad",Licenciado en Ciencia de la Computación,2019
Universidad Nacional de San Agustín de Arequipa,Modelo de visión artificial para la detección automática de colisión de vehículos en videovigilancia,"Laura Riveros, Elian Raquel","Gutierrez Caceres, Juan Carlos","La detección de objetos en el campo de Visión Artificial es todavía un reto con la finalidad de alcanzar la mejor precisión en el menor tiempo, enfrentándose a diversos factores del entorno real, tales como iluminación variable, oclusión parcial y otros objetos presentes. Aún continuán surgiendo técnicas de procesamiento y clasificación de imágenes para conformar modelos robustos de detección de objetos o acciones, una de las fortalezas de estos modelos está en la forma de operar con las características para obtener las que mejor representen el objeto y así obtener resultados satisfactorios de detección. Entre las técnicas de Aprendizaje de Máquina se encuentra la red neuronal artificial, que es un conjunto de cálculos matemáticos sobre las características del objeto a detectar. El número de neuronas y capas de una red neuronal artificial influye en los resultados de detección. El Aprendizaje Profundo (Deep Learning) es parte del amplio campo de Aprendizaje de Máquina que implica el uso de una gran cantidad de neuronas y muchas capas, este tipo de red consigue grandes resultados que se demuestran en diversos trabajos de investigación. El requerimiento de hardware es alto pero los avances tecnológicos han permitido la mejora de técnicas de deep learning. En la presente tesis se propone un modelo con una técnica de deep learning para la detección de carros y posteriormente se aplica una técnica de procesamiento de imágenes basado en el factor tiempo para detección de choques. Los resultados se demuestran con los tiempos de ejecución y el grado de precisión.",Ingeniera de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Reconocimiento de patrones de deserción de estudiantes universitarios basados en modelos de clasificación,"Zarate Valderrama, Ashley Joshua","Cornejo Aparicio, Victor Manuel","El trabajo estudia los factores que afectan el rendimiento académico de los estudiantes de la Universidad Nacional de San Agustín, las cuales incluyen características personales (género, edad), información previa a su ingreso a la universidad (tipo de colegio de procedencia, puntuación en examen de ingreso a la universidad) y el desempeño que posee en las asignaturas a lo largo de uno o más semestres académicos (promedio de notas por semestre, relación de créditos, nro. de asignaturas abandonadas, etc.). Para esto se ha creado una aplicación que utiliza información proporcionada por la universidad acerca de los estudiantes y que permite a los directores de las escuelas profesionales de la universidad el poder detectar de manera anticipada los estudiantes que presenten algún tipo de riesgo de abandonar la carrera. El reconocimiento de los patrones de deserción de los estudiantes es posible mediante el uso de modelos de clasificación generados a partir de diferentes algoritmos (redes neuronales, ID3, C4.5), utilizando los atributos más significantes dentro de la información que se tenía a la mano. Se ha creado modelos de clasificación por cada algoritmo que ha sido implementado y se realizaron pruebas utilizando diferentes variables de entrada por modelo. Cada modelo intenta detectar la deserción de un estudiante de su carrera profesional por lo que las variables de salida de cada uno serán: el estudiante ABANDONA y el estudiante NO ABANDONA la carrera. Se realizó también una comparación de estos modelos mediante el cálculo de medidas de rendimiento (precisión, exactitud y sensibilidad), y aquellos que ofrezcan mejores resultados serán aplicados cuando se realice la clasificación de estudiantes en las distintas escuelas profesionales de la universidad.",Ingeniero de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,"Modelo de una arquitectura basada en REST aplicada a la gestión de procesos en entidades públicas, caso: División de programación operativa de la administración tributaria - SUNAT","Tito Orozco, Edwin Rolando","Herrera Quispe, Jose Alfredo","Esta investigación propone un modelo de una arquitectura basada en REST aplicada a la gestión de procesos en entidades públicas, caso: División de Programación Operativa de la administración tributaria – SUNAT, para el desarrollo de funcionalidad específica; con atributos de agilidad, mantenibilidad y multiplataforma; resolviendo el problema de baja informatización de funcionalidad específica en entidades públicas. El modelado de procesos de negocio como herramienta para la gestión de procesos nos permite establecer las reglas del negocio, establecer las propuestas de mejora y finalmente fijar los requerimientos informáticos de negocio. La metodología planteada para la mejora de procesos en la presente investigación busca optimizar los procesos y/o procedimientos inmersos en las actividades desarrolladas por las entidades del aparato estatal. Se espera mejorar la fluidez de los procesos internos de la Superintendencia nacional de Aduanas y de administración tributaria (SUNAT) para los usuarios que utilicen la herramienta computacional; mejoren la calidad de su labor y se concentren en sus actividades especializadas para las cuales fueron contratados y por tanto permitan hacer más eficientes los procesos de la institución de cara al ciudadano. La arquitectura propuesta busca mejorar la productividad de construcción de software en el área desarrollo de TI en organismos públicos, contribuir en una mejor organización de la información para las organizaciones del medio que la usen",Ingeniero de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Small face detection using deep learning on surveillance videos,"Cardenas Talavera, Rolando Jesus","Gutierrez Caceres, Juan Carlos","La detección de rostros es una de las tareas esenciales ampliamente estudiadas en el campo de Visión Computacional. Varios autores han desarrollado diferentes técnicas para mejorar la detección de rostros en imágenes, pero estos trabajos están limitados a la hora de aplicarlos en videos y mas aun si estos presentan baja resolución. En esta investigación, se propone un nuevo modelo para la detección de rostros en videos de baja resolución basado en la morfología de la parte superior de las personas y utilizando las técnicas de Aprendizaje profundo (Redes convolucionales). Nuestros resultados muestran un promedio del 30% de precisión sobre la base de datos de Caviar y un 32% de precisión en la base de datos UCSP. Comparado con otras técnicas, nuestros resultados son mejores ya que trabajos anteriores solo alcanzan un 1% de precisión en estos escenarios.",Ingeniero de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,"Reconocimiento automático de habla aislado de números en Quechua usando MFCC, DTW AND KNN","Chacca Chuctaya, Hernan Faustino","Castro Gutierrez, Eveling Gloria","El área de reconocimiento automática de voz(ASR) se define como la transformación de señales acústicas en palabras de cadena. Esta área se ha desarrollado durante muchos años para facilitar la vida de las personas, por lo que se implementó en varios idiomas. Sin embargo, el desarrollo de ASR en algunos idiomas con pocos recursos de base de datos pero con una gran población que habla estos idiomas es muy bajo. El desarrollo de ASR en el idioma quechua es casi nulo, lo que lleva a la cultura y la población a aislarse de la tecnología y la información. En este trabajo se desarrolla un sistema ASR de números Quechua aislados donde se implementan los métodos de Mel-Frequency Cepstral Coefficients (MFCC), Dynamic Time Warping (DTW) y K-Nearest Neighbor (KNN) utilizando una base de datos compuesta por audios de números grabados en quechua desde uno hasta diez. Los audios grabados para alimentar la base de datos fueron grabados por hablantes nativos de quechua entre hombres y mujeres. La precisión de reconocimiento alcanzada en este trabajo de investigación fue de 91.1%.",Ingeniero de Sistemas,2019
Universidad Nacional de San Agustín de Arequipa,Detección de embarcaciones utilizando Deep Learning e imágenes satelitales ópticas,"Nina Choquehuayta, Wilder","Castro Gutierrez, Eveling Gloria","La detección de embarcaciones es un tema prioritario que ayuda a combatir la pesca ilegal, en búsqueda y rescate de navíos perdidos, entre otras actividades prioritarias en el mar Actualmente el uso técnicas de Aprendizaje Profundo en la detección de objetos está dando buenos resultados sobre imágenes satelitales. En la presente investigación se presenta un modelo que permite detectar embarcaciones dentro de las 100 millas del borde costero del Perú, utilizando técnicas de Aprendizaje Profundo e Imágenes Satelitales. Se realizó una comparación entre la última versión de You Only Look Once (YOLO) y You Only Look Twice (YOLT) para resolver el problema de detectar objetos pequeños (barcos) en el mar sobre imágenes satelitales ópticas debido a la gran diversidad de embarcaciones que existen en el Perú. Se trabajó con dos conjuntos de datos: High-Resolution Ship Collection (HRSC) y Mini Ship Data Set (MSDS), este último fue construido a partir de embarcaciones provenientes del borde costero del Perú. El ancho promedio de los objetos para HRSC y MSDS son 150 y 50 píxeles respectivamente. Los resultados mostraron que YOLT es bueno solo para objetos pequeños con 76,06% de Average Precision (AP), mientras que YOLO alcanzó 69,80 % en el conjunto de datos HRSC. Además, en el caso del conjunto de datos HRSC donde tienen objetos de diferentes tamaños, YOLT obtuvo un 40% de AP contra 75% de YOLO",Ingeniero de Sistemas,2020
Universidad Nacional de San Agustín de Arequipa,Detection of Suicidal Intent in Spanish Language Social Networks using Machine Learning,"Condori Larico, Alexia Katerine","Sulla Torres, José Alfredo","El suicidio es un problema considerable en nuestra población, la intervención temprana para su prevención tiene un papel muy importante, a fin de contrarrestar el número de muertes por suicidio. Hoy, poco más de la mitad de la población mundial utiliza las redes sociales, donde expresa ideas, sentimientos, deseos, incluso intenciones de suicidio. Motivado por estos factores, el objetivo principal es la detección automática de ideas suicidas en las redes sociales en idioma español, con el fin de que sirva como componente base para alertar y lograr intervenciones tempranas y especializadas. Para ello, se ha implementado un modelo de clasificación de frases suicidas en español, ya que actualmente no se encontraron trabajos relacionados en este idioma con un enfoque de aprendizaje automático. Sin embargo, hubo algunos desafíos al realizar esta tarea, como comprender el lenguaje natural, generar datos de entrenamiento y obtener una precisión confiable en la clasificación de estas frases. Para construir nuestro modelo de clasificación, se eligieron dos tipos opuestos y populares de incrustaciones de frases, y se compararon los algoritmos de clasificación más utilizados en la literatura. Obteniendo, como resultado, la confirmación de que es posible clasificar frases con ideación suicida en el idioma español con buena precisión utilizando representaciones semánticas.",Ingenieros de Sistemas,2020
Universidad Nacional de San Agustín de Arequipa,Virtual rehabilitation using sequential learning algorithms,"Calle Condori, Gladys Eliana","Castro Gutierrez, Eveling Gloria","Los sistemas de rehabilitación están cobrando mayor importancia en la actualidad debido a que los pacientes pueden tener acceso al tratamiento de recuperación de habilidades motoras desde el hogar, reduciendo las limitaciones de tiempo, espacio y costo del tratamiento en un centro médico. Los sistemas de rehabilitación tradicionales servían como guías de movimientos, posteriormente como espejos de movimiento y en los últimos años las investigaciones buscan generar mensajes de retroalimentación al paciente en base a la evaluación de sus movimientos. Actualmente los algoritmos más usados para la evaluación de ejercicios son Dynamic time warping (DTW), Hidden Markov model (HMM), Support vector machine (SVM). Sin embargo cuanto mayor es el conjunto de ejercicios a evaluar, disminuye la precisión del reconocimiento, generándose confusiones entre ejercicios que tienen descriptores de postura similares. En el presente trabajo de investigación se compara dos clasificadores HMM y Hidden Conditional Random Fields (HCRF) además de dos tipos de descriptores de la postura, basados en puntos y basados en ángulos de la postura. La representación por puntos demuestra ser superior a la representación por ángulos, sim embargo esta última aún es aceptable. Mientras que en HCRF y HMM se encuentran resultados similares.",Ingeniera de Sistemas,2020
Universidad Nacional de San Agustín de Arequipa,Modelo de sistema experto neuro-difuso para la codificación de conchas de abanico,"Perez Pinto, Roy Helbert Pepo","Portugal Zambrano, Christian Edilberto","La globalización afecta a todos los países sin importar que sean desarrollados, emergentes o en vías de desarrollo y los subdesarrollados. Para que un producto pueda ingresar en el mercado a nivel internacional debe vencer las altas barreras de la globalización y afrontar los estándares de calidad y producción que este exige. Los países en vías de desarrollo, como Perú [BIRF, 2019], tienen serias dificultades para cumplir con estos estándares de calidad mas aun cuando se tiene en frente a países desarrollados que cuentan con una producción optimizada al contar con procesos automatizados teniendo de esta forma un entorno con-trolado y poder cumplir un estándar específico. En el Perú existen productos que buscan su inserción en el mercado internacional entre ellos se tiene a la concha de abanico, la cual tiene alta demanda en Francia, Canadá y los Estados Unidos. La ciudad de Pisco, es una de las principales ciudades exportadoras en Perú de la concha de abanico, por ello surge la necesi-dad de automatizar los procesos de producción de conchas de abanico para poder insertar su producto en el mercado internacional, uno de estos procesos es el de la codificación de Conchas de Abanico el cual posee algunas singularidades, resaltando entre ellas el alto grado de subjetividad al momento de designar a un molusco con un grado de calidad (Código), en ocasiones se designa con un código de menor precio a moluscos que tienen probabilidades de ser clasificados con un código de mayor precio y viceversa, ocasionando pérdidas para alguna de las partes. En este trabajo, se presenta un modelo de sistema Experto Neurodi-fuso, como alternativa para la codificación de Conchas de Abanico, que permita obtener el código de clasificación establecido por la FAO al molusco, a partir de su peso en gramos. Adicionalmente este modelo da la opción de modificar manualmente las reglas del proceso de codificación, con el fin de realizar una codificación balanceada del producto.",Ingeniero de Sistemas,2020
Universidad Nacional de San Agustín de Arequipa,Classification of people who suffer schizophrenia and healthy people by EEG signals using Deep Learning,"Torres Naira, Carlos Alberto","Lopez Del Alamo, Cristian Jose","Más de 21 millones de personas en todo el mundo sufren de esquizofrenia. Este grave trastorno mental expone a las personas a estigmatización, discriminación y la violación de sus derechos humanos. Diferentes trabajos sobre clasificación y diagnóstico de enfermedades mentales usan señales de electroencefalograma (EEG), ya que refleja el funcionamiento del cerebro y cómo estas enfermedades lo afectan. Debido a la información proporcionada por las señales de EEG y el rendimiento demostrado por los algoritmos de Aprendizaje Profundo, el presente trabajo propone un modelo para la clasificación de personas esquizofrénicas y personas saludables a través de señales EEG utilizando métodos de Aprendizaje Profundo. Teniendo en cuenta las propiedades de un EEG, de alta dimensión y multicanal, aplicamos el coeficiente de correlación de Pearson (PCC) para representar las relaciones entre los canales, de esta manera, en lugar de utilizar la gran cantidad de datos que proporciona un EEG, utilizamos una matriz más corta como entrada de una red neuronal convolucional (CNN). Finalmente, los resultados demostraron que el modelo de clasificación basado en EEG propuesto logró una precisión, especificidad y sensibilidad del 90%, 90% y 90%, respectivamente.",Ingeniero de Sistemas,2020
Universidad Nacional de San Agustín de Arequipa,Sistema de detección de estudiantes en riesgo académico - SDERA,"Aruquipa Velazco, Gleny Danitza","Cornejo Aparicio, Victor Manuel","Es una patente de invención de procedimiento para sistema de detección de estudiantes en riesgo académico – SDERA comprende el proceso de Ingreso de Información Académica al sistema, Entrenar Modelos de Clasificación, Clasificación de estudiantes que permite construir un sistema de Detección de Estudiantes en Riesgo Académico, el mismo que en base a modelos de clasificación computacional, determine en base a data histórica; los patrones que identifiquen el riesgo académico, y con ello poder predecir si un estudiante se encuentra en riesgo de abandonar la carrera universitaria.",Ingenieros de Sistemas,2020
Universidad Nacional de San Agustín de Arequipa,Modelo de sistemas M-Learning para el reclutamiento y selección de talento humano / caso: Entrevistas,"Asto Machaca, Paula Carolina","Alfaro Casas, Luis Alberto","La entrevista dentro del reclutamiento y selección de talento humano es una actividad común realizada por las empresas con el fin de contratar el mejor personal que se ajuste a las características del puesto ofertado. Esta decision es de vital importancia para la organizacion reclutadora, ya que el empleado es una parte vital en el crecimiento de la institucion. A pesar esto, existe pocas e incluso nulas investigaciones fomentadas a mejorar el proceso y/o ensen˜anza con herramientas tecnológicas. El trabajo de investigacion titulado “Modelo de sistema M-learning para reclutamiento y seleccion de talento humano. Caso: Entrevistas” propone un sistema que ayude a la preparacion del candidato mediante el uso de un sistema mobile learning que ayude a incrementar los conocimientos que posee el usuario acerca de la entrevista y proporcionarle casos para que pueda practicar.",Ingeniera de Sistemas,2020
Universidad Nacional de San Agustín de Arequipa,Binarización híbrida para el degradado de imágenes de documentos históricos,"Yari Ramos, Yessenia Deysi","Gutierrez Caceres, Juan Carlos","Este trabajo revisó métodos que se encargan de realizar la binarización de documentos, los cuales podrían ser clasificados en dos, los que usan sólo técnicas de procesamiento de imágenes y los que utilizan inteligencia artificial para la resolución del problema. Se propone un método el cual binariza los documentos, usando sólo algoritmos de procesamiento de imágenes tales como Otsu, Sobel, Filtro de la mediana y operaciones morfológicas los cuales combinadas tienen un resultado de 0.92 (F ¡Mesure).",Ingeniero de Sistemas,2020
Universidad Nacional de San Agustín de Arequipa,Predicting Number of hospital Appointments when no data is available,"Caceres Zea, Harold Ernesto","Castro Gutierrez, Eveling Gloria","Por lo general, en un hospital, los datos generados por cada departamento o sección se tratan de forma aislada, creyendo que no existe una relación entre ellos. Se cree que si bien un departamento tiene una gran demanda, no puede influir en que otro pueda tener la misma demanda o no tener ninguna. En este documento, cuestionamos este enfoque al considerar la información de los departamentos como componentes de un gran sistema en el hospital. Por lo tanto, presentamos un algoritmo para predecir las citas de los departamentos cuando los datos no están disponibles utilizando datos de otros departamentos. Este algoritmo usa un modelo basado en regresión lineal múltiple usando una matriz de correlación para medir la relación entre los departamentos con diferentes ventanas de tiempo. Después de ejecutar nuestro algoritmo para diferentes ventanas de tiempo y departamentos, descubrimos experimentalmente que mientras aumentamos la extensión de una ventana de tiempo y aprendemos dependencias en los datos, su precisión correspondiente disminuye. De hecho, un mes de datos es el punto óptimo mínimo para aprovechar la información de otros departamentos y aún así proporcionar predicciones precisas. Estos resultados son importantes para desarrollar políticas de salud por departamento bajo datos limitados, un problema interesante que planeamos investigar en futuros trabajos.",Ingeniero de Sistemas,2020
Universidad Nacional de San Agustín de Arequipa,A heterogeneous scalable-orchestration architecture for home automation,"Apaza Condori, Jeferson Joel","Castro Gutierrez, Eveling Gloria","Internet de las cosas está representado por el gran número de dispositivos conectados a internet y esta cantidad de dispositivos está en constante crecimiento, de acuerdo con Gartner Inc. esta cifra llegaría a los 20,4 mil millones de dispositivos conectados para el 2020. Hay muchas industrias que hacen uso de esta tecnología entre ellas está la automatización de Hogares donde podemos encontrar una gran cantidad de propuestas arquitectónicas que intentan resolver la implementación de dispositivos conectados a internet en un ambiente o entorno, sin embargo la mayoría de estas propuestas solo dan soluciones para requisitos específicos usando una tecnología en concreta obviando problemas como la gestión de red, la seguridad, la escalabilidad de dispositivos heterogéneos, etc. En este trabajo presentamos una novedosa Arquitectura para la automatización de hogares basados en lineamientos propuestos por el ISO/IEC 30141 así como los casos de uso propuesto por el OneM2M, la implementación de esta Arquitectura nos garantizara una fácil integración, y orquestación de dispositivos IoT que se adaptan a un contexto ambiental, así como la gestión del control de las comunicaciones",Ingeniero de Sistemas,2020
Universidad Nacional de San Agustín de Arequipa,Data augmentation using generative adversarial network for gastrointestinal parasite microscopy image classification,"Pacompia Machaca, Mila Yoselyn","Castro Gutierrez, Eveling Gloria","Las enfermedades parasitarias gastrointestinales representan un problema latente en los países en desarrollo; es necesario crear herramientas de apoyo para el diagnóstico médico de estas enfermedades, se requiere automatizar tareas como la clasificación de muestras de los parásitos causantes obtenidas a través del microscopio utilizando métodos como el aprendizaje profundo. Sin embargo, estos métodos requieren grandes cantidades de datos. Actualmente, la recolección de estas imágenes representa un procedimiento complejo, importante consumo de recursos y largos períodos. Por tanto, es necesario proponer una solución computacional a este problema. En este trabajo se presenta un enfoque para generar conjuntos de imágenes sintéticas de 8 especies de parásitos, utilizando Redes Generativas Adversarias Convolucionales Profundas (DCGAN). Además, buscando mejores resultados, se aplicaron técnicas de mejora de imagen. Estos conjuntos de datos sintéticos (SD) fueron evaluados en una serie de combinaciones con los conjuntos de datos reales (RD) utilizando la tarea de clasificación, donde la mayor exactitud se obtuvo con el modelo Resnet50 pre-entrenado (99,2%), mostrando que el aumento de la RD con SD obtenido de DCGAN ayuda a lograr una mayor exactitud.",Ingenieras de Sistemas,2021
Universidad Nacional de San Agustín de Arequipa,Adapting competitiveness and gamification to a digital platform for foreign language,"Harvey Arce, Norman Patrick","Cuadros Valdivia, Ana Maria","Debido al crecimiento de la globalización, aprender un segundo idioma es una necesidad para desarrollarse en un entorno multicultural cada vez más exigente. Sin embargo, en la actualidad todavía nos encontramos con que se siguen aplicando algunas metodologías tradicionales para la docencia, lo que representa un problema para los estudiantes actuales, también llamados nativos digitales, porque estas metodologías deben adaptarse a esta era digital de la tecnología y el conocimiento. Para solucionar este problema, se implementó un recurso digital para el aprendizaje de lenguas extranjeras teniendo en cuenta la competitividad y la gamificación como bases para motivar a los estudiantes e involucrarlos en el curso. El objetivo de esta herramienta es mejorar el procesamiento de la información teórica obtenida en clase, haciendo uso de un entorno virtual que cuenta con actividades competitivas y elementos de gamificación como: obtención de medallas por realización de tareas, barra de progreso para cada usuario y un ranking según las puntuaciones obtenidas, para que los alumnos se sientan motivados y mejoren su aprendizaje. Esta investigación busca analizar los efectos de aplicar la competitividad y la gamificación en un entorno virtual orientado al aprendizaje de lenguas extranjeras.",Licenciado en Ciencia de la Computación,2021
Universidad Nacional de San Agustín de Arequipa,Modelo de sistema de soporte a la diagnosis de trastornos osteoarticulares de miembros inferiores utilizando procesamiento de imágenes de rayos X,"Castro Gutierrez, Eveling Gloria","Alfaro Casas, Luis Alberto","Los trastornos osteoarticulares aquejan a personas de todas las regiones del mundo sin distinción, ejemplos de ellas son: la osteoporosis y atrosis. La OMS determina la existencia de un incremento de casos en sociedades socioeconómicas más bajas y la Unión Europea establece una estrategia enfocada a entregar salud personalizada en el momento correcto, y brindar una alternativa de prevención oportuna y especifica denominada (PerMed). En este contexto nuestro país necesita aplicar la Medicina Personalizada para diagnosticar a tiempo enfermedades con alta incidencia. La presente investigación busca alinearse a los objetivos de la Medicina Personalizada proporcionando un modelo de sistema de soporte a la diagnosis de trastornos osteoarticulares de miembros inferiores utilizando procesamiento de imágenes de rayos X, teniendo presente la confidencialidad y protección de los datos. El pre-procesamiento de las imágenes de rayos X, permitió eliminar los desafíos de estas imágenes, y posibilito la generación de un gold-standard que sirvió como guía para la segmentación-registro de las estructuras óseas de miembros inferiores. Se utilizaron los modelos estadísticos como: SSM - Statistical Shape Model, SAM – Statistical Appeareance Model, ASM - Active Shape Model y Gradient Profiling en el refinamiento de la etapa de segmentación-registro como parte del entrenamiento y prueba. Estos modelos han sido validados con artículos de investigación presentados en el Capítulo IV con resultados de precisión en la segmentación entre el 74 % y 83 % y para la clasificación de las estructuras óseas dependiendo del objetivo a resolver sea: a) detectar fracturas en el acetábulo, o b) detectar osteoporosis en el fémur proximal, los resultados obtuvieron una precisión de: 73% y 87% respectivamente; y por ultimo para lograr el objetivo de: c) medir la distancia articular, se obtiene un error promedio equivalente a 2.4 px, este es un error aceptable para respaldar el diagnostico de desgaste articular de cadera llamado ""osteoartritis de cadera"". Asimismo, hubo una mejora significativa en el tiempo de procesamiento comparado con la literatura analizada.",Doctora en Ciencias de la Computación,2021
Universidad Nacional de San Agustín de Arequipa,Policies selection for pedagogical agent based on the roulette wheel algorithm,"Deza Veliz, David Alberto","Atencio Torres, Carlos Eduardo","Los agentes pedagógicos como entidades computacionales se implementan para interactuar con los usuarios y brindar oportunidades de aprendizaje, usualmente necesitan entrenamiento para seguir un conjunto de órdenes para un intercambio efectivo personalizado de conocimientos y tareas. En este estudio se propone evaluar la efectividad de un modelo basado en políticas y su nivel de satisfacción en su interacción de uso con y sin agente pedagógico. Utilizando el algoritmo bioinspirado de selección por ruleta. El enfoque es cuantitativo, con un estudio exploratorio y descriptivo. Los resultados revelaron que nuestro agente seleccionado con la estrategia de políticas logró una gran aceptación en sus usuarios que lo calificaron como inteligente, amigable y confiable. Como hallazgo se revela que el agente puede influir en las actitudes, percepciones y comportamiento de uso dado por el tiempo de permanencia que lo lleva a un aprendizaje autorregulado.",Ingeniero de Sistemas,2021
Universidad Nacional de San Agustín de Arequipa,A critical analysis of usability and learning methods on an augmented reality application for zoology education,"Flores Conislla, Michael Mario","Corrales Delgado, Carlo José Luis","En los últimos años, se han realizado investigaciones centradas en el uso de tecnologías en el aula, pero uno de los principales problemas es demostrar que el uso de esta tecnología favorece el aprendizaje frente a los métodos tradicionales. Una de estas tecnologías es la realidad aumentada que permite ver objetos virtuales superpuestos en el mundo real, pero para lograr su correcto uso es necesario evaluar la usabilidad que es la facilidad con la que se utiliza una interfaz. En este trabajo desarrollamos y analizamos desde dos perspectivas una aplicación educativa para la zoología que utiliza la realidad aumentada, una primera perspectiva es la usabilidad, donde se ha realizado un análisis de cómo el correcto diseño de una aplicación de realidad aumentada debe estar enfocándose en evaluaciones heurísticas, la segunda perspectiva es a nivel educativo donde la analizamos a nivel de aula donde medimos los diferentes métodos de aprendizaje (Aprendizaje Tradicional, Autoaprendizaje y Aprendizaje Guiado) y cuál se debe utilizar.",Ingeniero de Sistemas,2021
Universidad Nacional de San Agustín de Arequipa,New approaches and tools for ship detection in optical satellite imagery,"Avila Cordova, Aaron Walter","Castro Gutierrez, Eveling Gloria","La detección de barcos usando imágenes satelitales ópticas es una tarea muy importante para el campo de la seguridad marítima, ya sea en la búsqueda de embarcaciones perdidas o en el control marítimo de tipo comercial o militar. A esto se suman los avances en el campo de visión por computador, especialmente en el uso de modelos basados en inteligencia artificial, los cuales, permiten construir sistemas de detección robustos y más precisos. Sin embargo, los escenarios geográficos, propios de una imagen satelital, limitan el desarrollo de este tipo de sistemas ya que requieren de la disponibilidad de un gran número de imágenes en diferentes escenarios. En el presente trabajo se propone un nuevo enfoque para la Detección de Barcos uti- lizando dos nuevos conjuntos de datos etiquetados con cuadros delimitadores horizontales. Así mismo, se presenta una nueva herramienta de etiquetado (DATATOOL) que permite una mejor organización y distribución de los datos. Los nuevos conjuntos de datos, Peruvian Ship Dataset (PSDS) y Mini Ship Dataset (MSDS), han sido generados a partir de imágenes satelitales óp- ticas obtenidas de diferentes fuentes. El PSDS se crea a partir de 22 imágenes satelitales del PERUSAT-1 cuya resolución espacial es de 0.7m. Mientras que el MSDS ha sido generado uti- lizando imágenes satelitales provenientes de Google Earth dando un total de 1006 imágenes de 900x900 pixels. Las embarcaciones se encuentran tanto en el mar como en la costa. Finalmente se presentan los resultados de las pruebas utilizando algoritmos de aprendizaje profundo como YOLOv4 y YOLT, siguiendo el enfoque y las herramientas propuestas.",Ingeniero de Sistemas,2021
Universidad Nacional de San Agustín de Arequipa,Programación genética lineal aplicada a la síntesis automática de programas para sistemas basados en microcontrolador,"Ferrel Serruto, Wildor","Alfaro Casas, Luis Alberto","En la presente tesis se investiga la generación automática de programas de sistemas basados en microcontrolador empleando una metodología poco explorada como es la programación inductiva en la cual el punto de partida para la síntesis de un programa es una tabla de ejemplos de entrada-salida. La metodología propuesta se fundamenta en la aplicación de algoritmos de programación genética lineal. Un sistema basado en microcontrolador, dependiendo de su aplicación, incluye además del microcontrolador otros dispositivos periféricos como pulsadores, teclado matricial, display de indicadores de siete segmentos, pantalla LCD de texto, sensores, actuadores, etc. Estos dispositivos se conectan al microcontrolador a través de las líneas de entrada/salida y cada dispositivo tiene una forma particular de interacción y en algunos casos un protocolo complejo de comunicación con el microcontrolador. Se propone la aplicación de la programación genética lineal en la generación automática de rutinas de gestión de los dispositivos periféricos y de rutinas frecuentes, en la programación de microcontroladores, como rutinas de conversión de código o rutinas de implementación de máquinas de estados. La metodología propuesta se describe principalmente aplicada a la arquitectura 8051, pero se puede aplicar a otras arquitecturas, como se muestra en un ejemplo final ilustrativo, en el cual se generan automáticamente rutinas para la tarjeta Arduino Mega de arquitectura AVR. Para resolver el problema complejo de síntesis de los programas se han aplicado dos estrategias. En primer lugar, según el programa a sintetizar, en la evaluación de la aptiiii tud de un programa genético, se ha asignado una función, a maximizarse, a cada bit de resultado binario o al diagrama de tiempo de cada pin de un puerto del microcontrolador donde está conectado el dispositivo periférico, lo que ha permitido realizar una optimización multi-objetivo. En segundo lugar, se ha dividido un programa en lenguaje máquina en segmentos que evolucionan paralelamente colaborando entre sí, lo que significó la aplicación de un algoritmo de coevolución cooperativa. La metodología propuesta, en base a estas estrategias, ha permitido generar automáticamente programas, en algunos casos, con un tamaño de código más pequeño o un tiempo de ejecución más corto que los programas escritos por un programador humano.",Doctor en Ciencias de la Computación,2021
Universidad Nacional de San Agustín de Arequipa,Aplicabilidad de la interoperabilidad en el entorno sanitario,"Torres Zenteno, Arturo Henry","Castro Gutierrez, Eveling Gloria","La Interoperabilidad ha sido un denominador común en la mayoría de los proyectos en los que he participado en mi carrera profesional. Y he tenido la oportunidad de aplicarla en un contexto de mucho interés y relevancia, la Sanidad. El objetivo de la presentación de esta memoria es mostrar mi trayectoria a través de todos los proyectos donde he tenido la oportunidad de trabajar, resaltando, a nivel general, los principales conceptos que deben de tenerse en cuenta a la hora de integrar aplicaciones sanitarias. También se pretende mostrar cómo la aplicación de la interoperabilidad, en un contexto sanitario y con un enfoque adecuado, no sólo es importante, sino que es imprescindible de cara a conseguir una mejora en la calidad de atención del actor principal, el paciente. La memoria se centra y presenta detalle de un proyecto concreto de teleasistencia sobre el tratamiento del ictus y que está actualmente público y que ha sido publicado en Health Informatics Journal. Sin embargo, también se presenta un resumen general de proyectos de interés en materia de Interoperabilidad Sanitaria en donde he participado. Mi trayectoria profesional está basada principalmente en España, dentro de la empresa everis Spain S.L.U. Pero independientemente de esta realidad concreta, los conceptos y estándares sanitarios mencionados pueden ser aplicados en cualquier parte.",Ingeniero de Sistemas,2021
Universidad Nacional de San Agustín de Arequipa,Two - pass end-to-end: RNN-T-LAS VS. LSTM-LAS,"Escalante Calcina, Judith","Ramos Lovón, Wilber Roberto","El reconocimiento de voz se ha convertido en una funcionalidad obligatoria para diversos dispositivos y aplicaciones. Sin embargo, aún no se ha desarrollado la implementación más óptima que genere los mejores resultados. Por esta razón, en esta tesis se compararon los resultados de dos modelos end to end (E2E, por sus siglas en inglés) de dos pasos, en base a la latencia y usando la tasa de error por palabra (WER, por sus siglas en inglés). El primer modelo estuvo compuesto por una red neuronal recurrente transductora (RNN-T, por sus siglas en inglés) y una red escuchar, atender y deletrear (LAS, por sus siglas en inglés) y el segundo modelo se formó de una red de memoria de corto y largo plazo (LSTM, por sus siglas en inglés) con algunas alteraciones y una red LAS. Se empleó un enfoque comparativo, donde primero se replicó la implementación del modelo E2E de dos pasos integrado por una RNN-T y una red LAS; luego, se alteró la arquitectura del primer modelo para generar el segundo modelo E2E de dos pasos formado por una LSTM y una red LAS; y finalmente se realizaron experimentos para confrontar sus resultados en términos de latencia y usando el sistema WER. Los resultados de los experimentos basados en los modelos E2E de dos pasos muestran una tasa de error inferior que la producida por modelos convencionales (LAS, LSTM, RNN-T). Por otro lado, la latencia generada por el modelo LSTM - LAS es menor que la producida por el modelo RNN-T - LAS, señalando de esta forma que las modificaciones realizadas para elaborar el segundo modelo fueron productivas. Por último, la evaluación hecha para medir el rendimiento expuso un buen desempeño, pero también revela que ambos modelos E2E de dos pasos tienen una alta tasa de pérdida. Las comparaciones entre modelos son importantes, ya que contribuyen a la optimización en los resultados y además pueden motivar la generación de nuevas propuestas, arquitecturas e incluso modelos E2E.",Licenciada en Ciencia de la Computación,2021
Universidad Nacional de San Agustín de Arequipa,Clasificación de cerámicas arqueológicas 3D usando multi-view geodesic farthest point sampling,"Lazo Colque, Patrick Anthony","Lopez Del Alamo, Cristian Jose","La clasificación de cerámicas arqueológicas es un aspecto clave en la arqueología, ya que permite identificar los diferentes tipos de cerámicas y, posteriormente, interpretar sus funciones o usos que tuvieron en la antigua sociedad. Gracias al desarrollo de tecnologías de adquisición 3D como sensores y a los esfuerzos en computer vision (CV) y deep learning es posible diseñar un método inteligente capaz de automatizar la clasificación de cerámicas arqueológicas. En la presente tesis, se propone un descriptor similar a Multiview-Curvature denominado Multiview-Geodesic para la clasificación de cerámicas arqueológicas. Multiview-Geodesic está basado en la combinación de los mapas de distancias geodésicas generadas desde varias fuentes con las múltiples vistas de las cerámicas arqueológicas. El descriptor propuesto fue probado en dos conjuntos de cerámicas arqueológicas (a) Peruvian Dataset representada por 938 cerámicas arqueológicas recolectadas de diferentes museos en la ciudad de Lima-Peru, y (b) 3D Pottery dataset representada por 411 cerámicas, obteniendo la mejor puntuación tanto en Peruvian dataset con 82,67\% como en 3D Pottery dataset con 97,56 %.",Licenciado en Ciencia de la Computación,2021
Universidad Nacional de San Agustín de Arequipa,Modelo de pronóstico para la programación de despachos usando inteligencia artificial,"Romero Rodriguez, Wilber Benjamin","Huertas Niquen, Percy Oscar","Luego de una revisión sobre los costos y tiempos vinculados a la programación de despachos en una empresa concretera, se evidencian sobrecostos derivados de los altos tiempos de espera para la entrega del producto (inicio del descargue en obra), esto debido a un modelo de programación empírico y basado en la experiencia del operador para el despacho de clientes. En este contexto se propone un nuevo modelo de despacho usando inteligencia artificial para mejorar la estimación del tiempo de espera y vaciado. El modelo de inteligencia artificial se basa en el razonamiento basado en casos, donde se utiliza una estructura de datos de acceso secuencial la que permite la indexación en memoria de todos los casos facilitando tareas de recuperación de datos. Una mejor planificación reducirá los tiempos muertos, reduciendo los sobrecostos y mejorando la rentabilidad de la empresa. Por la confidencialidad de los datos la empresa del caso de estudios será llamada: empresa Concretera S.A.",Ingeniero de Sistemas,2021
Universidad Nacional de San Agustín de Arequipa,Document classification method based on graphs and concepts of non-rigid 3D models approach,"Castillo Galdos, Lorena Xiomara","López del Alamo, Cristian José","La clasificación de documentos de texto es un tema de investigación importante en el campo de recuperación de información, así como la manera en la que representamos la información extraída de los documentos a ser clasificados. Existen métodos y técnicas de clasificación de documentos basadas en el modelo espacio vectorial, el cual no captura la relación entre palabras, la cual se considera importante para realizar una mejor comparación y por lo tanto clasificación. Es por esto que en este trabajo de investigación dos aportes significativos fueron desarrollados, el primero es la obtención de vectores característica, los cuales están basados en un enfoque de comparación de documentos por similaridad, el cual utiliza conceptos adaptados de comparación de modelos 3D no rígidos y grafos como estructura de datos para la representación de dichos documentos. El segundo aporte es el método de clasificación, el cual está basado en la obtención de vectores característica representativos de cada clase de documentos.",Licenciada en Ciencia de la Computación,2021
Universidad Nacional de San Agustín de Arequipa,Identificación biométrica única para habilitar la interoperabilidad en el Perú utilizando deep learning y bigdata,"Benavides Esquivel, Jaime","Huertas Niquen, Percy Oscar","El campo del reconocimiento facial ha experimentado avances constantes en las dos últimas décadas, hasta llegar a los precisos sistemas de reconocimiento en tiempo real disponibles hoy en día en los teléfonos móviles y en las redes sociales, Actualmente la inteligencia artificial ha evolucionado tanto, que ahora existe hasta como servicio Amazon Rekognition , así como Face ID de Apple. Esta tesis propone una solución de identidad biométrica única, una solución importante en nuestro país, puesto que nos va permitir identificar a una persona en entornos digitales, verificando digitalmente a la persona quien dice ser, y con esta identificación poder realizar trámites en línea, sin la necesidad de ir personalmente a una oficina de gobierno, habilitando así una sociedad y una economía digital y dejando una trazabilidad o huella digital. La identificación de la identidad biométrica única, es una solución importante para que un ciudadano que vive en cualquier parte del país, pueda realizar un trámite en línea y de forma remota, utilizando video cámaras con lo que ayudara al funcionario de gobierno atenderlo en línea y de forma remota, ahorrando costos y tiempos en atención utilizando la ventanilla única digital. El estado peruano ha planteado una ley para la identidad digital única, y tiene como objetivo permitir la autenticación en línea de la identidad de las personas peruanas y extranjeras cuando necesiten acceder a los servicios digitales brindados por las entidades públicas, esta ley fue publicado en septiembre del 2018, con el decreto legislativo nº 1412, en el capítulo II identidad digital, lo cual impulsa la implementación de la plataforma nacional de identificación y autenticación de la identidad digital. La tesis presentada tiene un alcance propuesto desde el registro biométrico, y la generación de la credencial de la identidad digital, así como los mecanismos de autenticación y autorización digital, utilizando reconocimiento biométrico y firmas digitales, para habilitar una interoperabilidad digital [5] utilizando X-Road . Esta tesis propone una solución de identidad biométrica única utilizando bibliotecas de identificación de rostro con OpenCV [1], así como para el reconocimiento de rostros utilizando técnicas de aprendizaje profundo, y aprendizaje por transferencia. Se realizó comparaciones con las diferentes arquitecturas VGG16[2], VGG19[2] y RESTNET50 [3] generando una precisión de entrenamiento mayor al 98%, trabajando con 9 clases y cada clase con 300 imágenes de 224x224 pixeles por imagen de entrada para cada clase, apoyados de las bibliotecas de TensorFlow, Keras, Sklearn, y validando el desempeño de los modelos utilizando la matriz de confusión, así como para su almacenamiento de la trazabilidad de identificación y autenticación en BigData, y la utilización de contenedores (kubernetes/docker) para tener un nivel de escalamiento nacional y global. Cada una de estas bibliotecas y la propuesta de solución son 100% software libre.",Ingeniero de Sistemas,2022
Universidad Nacional de San Agustín de Arequipa,Hybrid model of quatum transfer learning to classify faces images with a Covid-19 mask,"Soto Paredes, Christian Jaime","Sulla Torres, José Alfredo","Según estadísticas a la fecha, se estableció que la enfermedad conocida como Covid-19 fue contraída por 219 millones de personas, de las cuales 4.55 millones concluyeron en muerte. Es así que para prevenir el avance de la enfermedad se establecieron protocolos de seguridad. Uno de los principales protocolos es el que establece el uso de mascarillas protectoras las cuales deben cubrir adecuadamente nariz y boca. Es en ese sentido que el objetivo de este artículo fue clasificar imágenes de rostros usando mascarillas protectoras del Covid-19, en las clases identificadas como: correct mask, incorrect mask y no mask; con un modelo Hibrido de Quantum Transfer Learning, utilizando para tal fin un dataset de 660 personas de ambos sexos (masculino y femenino), con edades de entre los 18 a 86 a˜nos. El modelo de transfer learning clásico escogido fue ResNet-18, las capas variacionales del modelo propuesto se construyeron con el template Basic Entagler Layers para 4 qubits y la optimización del entrenamiento se realizó con el Stochastic Gradient Descent con Nesterov Momentum. En las pruebas realizadas se obtuvo un 99.05% de exactitud en la clasificación utilizando el simulador cuántico de Pennylane. Es así que se concluye que el modelo hibrido propuesto es una excelente opción para detectar correctamente la posición de las mascarillas protectoras del Covid-19.",Ingeniero de Sistemas,2022
Universidad Nacional de San Agustín de Arequipa,Reconocimiento de imagen por medio de dispositivos móviles para la rehabilitación de la lesión del ligamento cruzado anterior de la rodilla,"Contreras Alcazar, Iam Fabrizio","Cornejo Aparicio, Victor Manuel","La lesión del ligamento cruzado anterior es una afección que requiere terapia de rehabilitación física. Debido a la problemática de la pandemia por Covid-19 y los problemas de movilidad del paciente, es complicado asistir a las sesiones de rehabilitación. El aplicativo móvil desarrollado utiliza el reconocimiento de colores por medio de la librería OpenCV, con ello se puede generar un goniómetro virtual al captar los puntos anatómicos específicos del miembro inferior a través de la cámara del dispositivo. Para activarlo, es necesario decir comandos específicos de voz, con esto el aplicativo inicia el reconocimiento o se detiene. Permite, además, controlar y hacer seguimiento de los ejercicios prescritos por un especialista. Los ejercicios efectuados por el paciente son registrados por el aplicativo móvil el cual captura las series y repeticiones, los movimientos de flexión y extensión, y sus ángulos máximos y mínimos respectivamente; gracias a ello se puede realizar un correcto seguimiento de su desempeño. Se obtuvieron los resultados de cuatro sujetos de prueba de diferentes edades y sexos al someterlos a ejercicios de rehabilitación y registrar sus mediciones respectivas, de esta forma se pudo comprobar la efectividad de la aplicación.",Ingeniero de Sistemas,2022
